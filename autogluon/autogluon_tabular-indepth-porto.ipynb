{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0585d88",
   "metadata": {},
   "source": [
    "https://auto.gluon.ai/0.1.0/tutorials/tabular_prediction/tabular-indepth.html\n",
    "    \n",
    "# Predicting Columns in a Table - In Depth: porto_seguro (classification)\n",
    "\n",
    "Tip: If you are new to AutoGluon, review \"Predicting Columns in a Table - Quick Start\" to learn the basics of the AutoGluon API.\n",
    "\n",
    "https://auto.gluon.ai/0.1.0/tutorials/tabular_prediction/tabular-quickstart.html#sec-tabularquick\n",
    "\n",
    "This tutorial describes how you can exert greater control when using AutoGluon’s `fit()` or `predict()`. Recall that to maximize predictive performance, you should always first try `fit()` with all default arguments except `eval_metric` and `presets`, before you experiment with other arguments covered in this in-depth tutorial like `hyperparameter_tune_kwargs`, `hyperparameters`, `num_stack_levels`, `num_bag_folds`, `num_bag_sets`, etc.\n",
    "\n",
    "Using the same census data table as in the \"Predicting Columns in a Table - Quick Start\" tutorial, we’ll now predict the occupation of an individual - a multiclass classification problem. Start by importing AutoGluon’s `TabularPredictor` and `TabularDataset`, and loading the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0b147220",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca944576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data = TabularDataset('https://autogluon.s3.amazonaws.com/datasets/Inc/train.csv')\n",
    "train_data = TabularDataset(\"porto_train.csv\")\n",
    "test_data = TabularDataset(\"porto_test.csv\")\n",
    "# subsample_size = 500  # subsample subset of data for faster demo, try setting this to much larger values\n",
    "# train_data = train_data.sample(n=subsample_size, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f7958f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'autogluon.core.dataset.TabularDataset'>\n",
      "RangeIndex: 476170 entries, 0 to 476169\n",
      "Data columns (total 60 columns):\n",
      " #   Column          Non-Null Count   Dtype  \n",
      "---  ------          --------------   -----  \n",
      " 0   id              476170 non-null  int64  \n",
      " 1   target          476170 non-null  int64  \n",
      " 2   ps_ind_01       476170 non-null  int64  \n",
      " 3   ps_ind_02_cat   476170 non-null  int64  \n",
      " 4   ps_ind_03       476170 non-null  int64  \n",
      " 5   ps_ind_04_cat   476170 non-null  int64  \n",
      " 6   ps_ind_05_cat   476170 non-null  int64  \n",
      " 7   ps_ind_06_bin   476170 non-null  int64  \n",
      " 8   ps_ind_07_bin   476170 non-null  int64  \n",
      " 9   ps_ind_08_bin   476170 non-null  int64  \n",
      " 10  ps_ind_09_bin   476170 non-null  int64  \n",
      " 11  ps_ind_10_bin   476170 non-null  int64  \n",
      " 12  ps_ind_11_bin   476170 non-null  int64  \n",
      " 13  ps_ind_12_bin   476170 non-null  int64  \n",
      " 14  ps_ind_13_bin   476170 non-null  int64  \n",
      " 15  ps_ind_14       476170 non-null  int64  \n",
      " 16  ps_ind_15       476170 non-null  int64  \n",
      " 17  ps_ind_16_bin   476170 non-null  int64  \n",
      " 18  ps_ind_17_bin   476170 non-null  int64  \n",
      " 19  ps_ind_18_bin   476170 non-null  int64  \n",
      " 20  ps_reg_01       476170 non-null  float64\n",
      " 21  ps_reg_02       476170 non-null  float64\n",
      " 22  ps_reg_03       476170 non-null  float64\n",
      " 23  ps_car_01_cat   476170 non-null  int64  \n",
      " 24  ps_car_02_cat   476170 non-null  int64  \n",
      " 25  ps_car_03_cat   476170 non-null  int64  \n",
      " 26  ps_car_04_cat   476170 non-null  int64  \n",
      " 27  ps_car_05_cat   476170 non-null  int64  \n",
      " 28  ps_car_06_cat   476170 non-null  int64  \n",
      " 29  ps_car_07_cat   476170 non-null  int64  \n",
      " 30  ps_car_08_cat   476170 non-null  int64  \n",
      " 31  ps_car_09_cat   476170 non-null  int64  \n",
      " 32  ps_car_10_cat   476170 non-null  int64  \n",
      " 33  ps_car_11_cat   476170 non-null  int64  \n",
      " 34  ps_car_11       476170 non-null  int64  \n",
      " 35  ps_car_12       476170 non-null  float64\n",
      " 36  ps_car_13       476170 non-null  float64\n",
      " 37  ps_car_14       476170 non-null  float64\n",
      " 38  ps_car_15       476170 non-null  float64\n",
      " 39  ps_calc_01      476170 non-null  float64\n",
      " 40  ps_calc_02      476170 non-null  float64\n",
      " 41  ps_calc_03      476170 non-null  float64\n",
      " 42  ps_calc_04      476170 non-null  int64  \n",
      " 43  ps_calc_05      476170 non-null  int64  \n",
      " 44  ps_calc_06      476170 non-null  int64  \n",
      " 45  ps_calc_07      476170 non-null  int64  \n",
      " 46  ps_calc_08      476170 non-null  int64  \n",
      " 47  ps_calc_09      476170 non-null  int64  \n",
      " 48  ps_calc_10      476170 non-null  int64  \n",
      " 49  ps_calc_11      476170 non-null  int64  \n",
      " 50  ps_calc_12      476170 non-null  int64  \n",
      " 51  ps_calc_13      476170 non-null  int64  \n",
      " 52  ps_calc_14      476170 non-null  int64  \n",
      " 53  ps_calc_15_bin  476170 non-null  int64  \n",
      " 54  ps_calc_16_bin  476170 non-null  int64  \n",
      " 55  ps_calc_17_bin  476170 non-null  int64  \n",
      " 56  ps_calc_18_bin  476170 non-null  int64  \n",
      " 57  ps_calc_19_bin  476170 non-null  int64  \n",
      " 58  ps_calc_20_bin  476170 non-null  int64  \n",
      " 59  fold            476170 non-null  int64  \n",
      "dtypes: float64(10), int64(50)\n",
      "memory usage: 218.0 MB\n"
     ]
    }
   ],
   "source": [
    "train_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "221adb42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'autogluon.core.dataset.TabularDataset'>\n",
      "RangeIndex: 119042 entries, 0 to 119041\n",
      "Data columns (total 59 columns):\n",
      " #   Column          Non-Null Count   Dtype  \n",
      "---  ------          --------------   -----  \n",
      " 0   id              119042 non-null  int64  \n",
      " 1   target          119042 non-null  int64  \n",
      " 2   ps_ind_01       119042 non-null  int64  \n",
      " 3   ps_ind_02_cat   119042 non-null  int64  \n",
      " 4   ps_ind_03       119042 non-null  int64  \n",
      " 5   ps_ind_04_cat   119042 non-null  int64  \n",
      " 6   ps_ind_05_cat   119042 non-null  int64  \n",
      " 7   ps_ind_06_bin   119042 non-null  int64  \n",
      " 8   ps_ind_07_bin   119042 non-null  int64  \n",
      " 9   ps_ind_08_bin   119042 non-null  int64  \n",
      " 10  ps_ind_09_bin   119042 non-null  int64  \n",
      " 11  ps_ind_10_bin   119042 non-null  int64  \n",
      " 12  ps_ind_11_bin   119042 non-null  int64  \n",
      " 13  ps_ind_12_bin   119042 non-null  int64  \n",
      " 14  ps_ind_13_bin   119042 non-null  int64  \n",
      " 15  ps_ind_14       119042 non-null  int64  \n",
      " 16  ps_ind_15       119042 non-null  int64  \n",
      " 17  ps_ind_16_bin   119042 non-null  int64  \n",
      " 18  ps_ind_17_bin   119042 non-null  int64  \n",
      " 19  ps_ind_18_bin   119042 non-null  int64  \n",
      " 20  ps_reg_01       119042 non-null  float64\n",
      " 21  ps_reg_02       119042 non-null  float64\n",
      " 22  ps_reg_03       119042 non-null  float64\n",
      " 23  ps_car_01_cat   119042 non-null  int64  \n",
      " 24  ps_car_02_cat   119042 non-null  int64  \n",
      " 25  ps_car_03_cat   119042 non-null  int64  \n",
      " 26  ps_car_04_cat   119042 non-null  int64  \n",
      " 27  ps_car_05_cat   119042 non-null  int64  \n",
      " 28  ps_car_06_cat   119042 non-null  int64  \n",
      " 29  ps_car_07_cat   119042 non-null  int64  \n",
      " 30  ps_car_08_cat   119042 non-null  int64  \n",
      " 31  ps_car_09_cat   119042 non-null  int64  \n",
      " 32  ps_car_10_cat   119042 non-null  int64  \n",
      " 33  ps_car_11_cat   119042 non-null  int64  \n",
      " 34  ps_car_11       119042 non-null  int64  \n",
      " 35  ps_car_12       119042 non-null  float64\n",
      " 36  ps_car_13       119042 non-null  float64\n",
      " 37  ps_car_14       119042 non-null  float64\n",
      " 38  ps_car_15       119042 non-null  float64\n",
      " 39  ps_calc_01      119042 non-null  float64\n",
      " 40  ps_calc_02      119042 non-null  float64\n",
      " 41  ps_calc_03      119042 non-null  float64\n",
      " 42  ps_calc_04      119042 non-null  int64  \n",
      " 43  ps_calc_05      119042 non-null  int64  \n",
      " 44  ps_calc_06      119042 non-null  int64  \n",
      " 45  ps_calc_07      119042 non-null  int64  \n",
      " 46  ps_calc_08      119042 non-null  int64  \n",
      " 47  ps_calc_09      119042 non-null  int64  \n",
      " 48  ps_calc_10      119042 non-null  int64  \n",
      " 49  ps_calc_11      119042 non-null  int64  \n",
      " 50  ps_calc_12      119042 non-null  int64  \n",
      " 51  ps_calc_13      119042 non-null  int64  \n",
      " 52  ps_calc_14      119042 non-null  int64  \n",
      " 53  ps_calc_15_bin  119042 non-null  int64  \n",
      " 54  ps_calc_16_bin  119042 non-null  int64  \n",
      " 55  ps_calc_17_bin  119042 non-null  int64  \n",
      " 56  ps_calc_18_bin  119042 non-null  int64  \n",
      " 57  ps_calc_19_bin  119042 non-null  int64  \n",
      " 58  ps_calc_20_bin  119042 non-null  int64  \n",
      "dtypes: float64(10), int64(49)\n",
      "memory usage: 53.6 MB\n"
     ]
    }
   ],
   "source": [
    "test_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e448e2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <td>9.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>20.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_ind_01</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_ind_02_cat</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_ind_03</th>\n",
       "      <td>7.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_ind_04_cat</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_ind_05_cat</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_ind_06_bin</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_ind_07_bin</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_ind_08_bin</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_ind_09_bin</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_ind_10_bin</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_ind_11_bin</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_ind_12_bin</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_ind_13_bin</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_ind_14</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_ind_15</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_ind_16_bin</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_ind_17_bin</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_ind_18_bin</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_reg_01</th>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_reg_02</th>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_reg_03</th>\n",
       "      <td>0.766078</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.580948</td>\n",
       "      <td>0.840759</td>\n",
       "      <td>0.617454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_car_01_cat</th>\n",
       "      <td>11.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_car_02_cat</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_car_03_cat</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_car_04_cat</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_car_05_cat</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_car_06_cat</th>\n",
       "      <td>11.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>11.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_car_07_cat</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_car_08_cat</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_car_09_cat</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_car_10_cat</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_car_11_cat</th>\n",
       "      <td>19.000000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>104.000000</td>\n",
       "      <td>82.000000</td>\n",
       "      <td>99.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_car_11</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_car_12</th>\n",
       "      <td>0.316228</td>\n",
       "      <td>0.316228</td>\n",
       "      <td>0.374166</td>\n",
       "      <td>0.316070</td>\n",
       "      <td>0.316228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_car_13</th>\n",
       "      <td>0.618817</td>\n",
       "      <td>0.641586</td>\n",
       "      <td>0.542949</td>\n",
       "      <td>0.565832</td>\n",
       "      <td>0.639683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_car_14</th>\n",
       "      <td>0.388716</td>\n",
       "      <td>0.347275</td>\n",
       "      <td>0.294958</td>\n",
       "      <td>0.365103</td>\n",
       "      <td>0.368782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_car_15</th>\n",
       "      <td>2.449490</td>\n",
       "      <td>3.316625</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.162278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_calc_01</th>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_calc_02</th>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_calc_03</th>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_calc_04</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_calc_05</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_calc_06</th>\n",
       "      <td>9.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_calc_07</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_calc_08</th>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_calc_09</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_calc_10</th>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_calc_11</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_calc_12</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_calc_13</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_calc_14</th>\n",
       "      <td>9.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_calc_15_bin</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_calc_16_bin</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_calc_17_bin</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_calc_18_bin</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_calc_19_bin</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_calc_20_bin</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fold</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        0          1           2          3          4\n",
       "id               9.000000  13.000000   16.000000  17.000000  20.000000\n",
       "target           0.000000   0.000000    0.000000   0.000000   0.000000\n",
       "ps_ind_01        1.000000   5.000000    0.000000   0.000000   2.000000\n",
       "ps_ind_02_cat    1.000000   4.000000    1.000000   2.000000   1.000000\n",
       "ps_ind_03        7.000000   9.000000    2.000000   0.000000   3.000000\n",
       "ps_ind_04_cat    0.000000   1.000000    0.000000   1.000000   1.000000\n",
       "ps_ind_05_cat    0.000000   0.000000    0.000000   0.000000   0.000000\n",
       "ps_ind_06_bin    0.000000   0.000000    1.000000   1.000000   0.000000\n",
       "ps_ind_07_bin    0.000000   0.000000    0.000000   0.000000   1.000000\n",
       "ps_ind_08_bin    1.000000   1.000000    0.000000   0.000000   0.000000\n",
       "ps_ind_09_bin    0.000000   0.000000    0.000000   0.000000   0.000000\n",
       "ps_ind_10_bin    0.000000   0.000000    0.000000   0.000000   0.000000\n",
       "ps_ind_11_bin    0.000000   0.000000    0.000000   0.000000   0.000000\n",
       "ps_ind_12_bin    0.000000   0.000000    0.000000   0.000000   0.000000\n",
       "ps_ind_13_bin    0.000000   0.000000    0.000000   0.000000   0.000000\n",
       "ps_ind_14        0.000000   0.000000    0.000000   0.000000   0.000000\n",
       "ps_ind_15        3.000000  12.000000    8.000000   9.000000   8.000000\n",
       "ps_ind_16_bin    0.000000   1.000000    1.000000   1.000000   1.000000\n",
       "ps_ind_17_bin    0.000000   0.000000    0.000000   0.000000   0.000000\n",
       "ps_ind_18_bin    1.000000   0.000000    0.000000   0.000000   0.000000\n",
       "ps_reg_01        0.800000   0.000000    0.900000   0.700000   0.600000\n",
       "ps_reg_02        0.400000   0.000000    0.200000   0.600000   0.100000\n",
       "ps_reg_03        0.766078  -1.000000    0.580948   0.840759   0.617454\n",
       "ps_car_01_cat   11.000000   7.000000    7.000000  11.000000   6.000000\n",
       "ps_car_02_cat    1.000000   1.000000    1.000000   1.000000   1.000000\n",
       "ps_car_03_cat   -1.000000  -1.000000    0.000000  -1.000000  -1.000000\n",
       "ps_car_04_cat    0.000000   0.000000    0.000000   0.000000   0.000000\n",
       "ps_car_05_cat   -1.000000  -1.000000    1.000000  -1.000000   1.000000\n",
       "ps_car_06_cat   11.000000  14.000000   11.000000  14.000000  11.000000\n",
       "ps_car_07_cat    1.000000   1.000000    1.000000   1.000000   1.000000\n",
       "ps_car_08_cat    1.000000   1.000000    1.000000   1.000000   1.000000\n",
       "ps_car_09_cat    2.000000   2.000000    3.000000   2.000000   0.000000\n",
       "ps_car_10_cat    1.000000   1.000000    1.000000   1.000000   1.000000\n",
       "ps_car_11_cat   19.000000  60.000000  104.000000  82.000000  99.000000\n",
       "ps_car_11        3.000000   1.000000    1.000000   3.000000   2.000000\n",
       "ps_car_12        0.316228   0.316228    0.374166   0.316070   0.316228\n",
       "ps_car_13        0.618817   0.641586    0.542949   0.565832   0.639683\n",
       "ps_car_14        0.388716   0.347275    0.294958   0.365103   0.368782\n",
       "ps_car_15        2.449490   3.316625    2.000000   2.000000   3.162278\n",
       "ps_calc_01       0.300000   0.500000    0.600000   0.400000   0.200000\n",
       "ps_calc_02       0.100000   0.700000    0.900000   0.600000   0.600000\n",
       "ps_calc_03       0.300000   0.100000    0.100000   0.000000   0.500000\n",
       "ps_calc_04       2.000000   2.000000    2.000000   2.000000   2.000000\n",
       "ps_calc_05       1.000000   2.000000    4.000000   2.000000   2.000000\n",
       "ps_calc_06       9.000000   9.000000    7.000000   6.000000   8.000000\n",
       "ps_calc_07       5.000000   1.000000    1.000000   3.000000   1.000000\n",
       "ps_calc_08       8.000000   8.000000    8.000000  10.000000   8.000000\n",
       "ps_calc_09       1.000000   2.000000    4.000000   2.000000   3.000000\n",
       "ps_calc_10       7.000000   7.000000    2.000000  12.000000  10.000000\n",
       "ps_calc_11       3.000000   4.000000    2.000000   3.000000   3.000000\n",
       "ps_calc_12       1.000000   2.000000    2.000000   1.000000   0.000000\n",
       "ps_calc_13       1.000000   7.000000    4.000000   1.000000   0.000000\n",
       "ps_calc_14       9.000000   7.000000    9.000000   3.000000  10.000000\n",
       "ps_calc_15_bin   0.000000   0.000000    0.000000   0.000000   0.000000\n",
       "ps_calc_16_bin   1.000000   1.000000    0.000000   0.000000   1.000000\n",
       "ps_calc_17_bin   1.000000   1.000000    0.000000   0.000000   0.000000\n",
       "ps_calc_18_bin   0.000000   0.000000    0.000000   1.000000   0.000000\n",
       "ps_calc_19_bin   1.000000   1.000000    0.000000   1.000000   1.000000\n",
       "ps_calc_20_bin   0.000000   0.000000    0.000000   0.000000   0.000000\n",
       "fold             5.000000   1.000000    3.000000   2.000000   1.000000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d28d79ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of target column: \n",
      " count    476170.000000\n",
      "mean          0.036455\n",
      "std           0.187421\n",
      "min           0.000000\n",
      "25%           0.000000\n",
      "50%           0.000000\n",
      "75%           0.000000\n",
      "max           1.000000\n",
      "Name: target, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "label = 'target'\n",
    "print(\"Summary of target column: \\n\", train_data[label].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903485f1",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7b2c1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.replace(-1, np.nan)\n",
    "test_data = test_data.replace(-1, np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5fb6c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.drop([\"id\", \"fold\"], axis=1)\n",
    "test_data = test_data.drop([\"id\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a46dcc76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(476170, 58)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25fde940",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(119042, 58)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "80975f43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ps_ind_02_cat',\n",
       " 'ps_ind_04_cat',\n",
       " 'ps_ind_05_cat',\n",
       " 'ps_car_01_cat',\n",
       " 'ps_car_02_cat',\n",
       " 'ps_car_03_cat',\n",
       " 'ps_car_04_cat',\n",
       " 'ps_car_05_cat',\n",
       " 'ps_car_06_cat',\n",
       " 'ps_car_07_cat',\n",
       " 'ps_car_08_cat',\n",
       " 'ps_car_09_cat',\n",
       " 'ps_car_10_cat',\n",
       " 'ps_car_11_cat']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_vars = [col for col in train_data.columns if 'cat' in col]\n",
    "cat_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "16d806a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in cat_vars:\n",
    "    test_data[col] = test_data[col].astype('category')\n",
    "    \n",
    "cat_vars = cat_vars + [\"target\"]\n",
    "\n",
    "for col in cat_vars:\n",
    "    train_data[col] = train_data[col].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8fb7b506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_data = TabularDataset('https://autogluon.s3.amazonaws.com/datasets/Inc/test.csv')\n",
    "# test_data = new_data[5000:].copy()  # this should be separate data in your applications\n",
    "y_test = test_data[label]\n",
    "test_data_nolabel = test_data.drop(columns=[label])  # delete label column\n",
    "# val_data = new_data[:5000].copy()\n",
    "\n",
    "# metric = 'accuracy' # we specify eval-metric just for demo (unnecessary as it's the default)\n",
    "metric = 'roc_auc' # we specify eval-metric just for demo (unnecessary as it's the default)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5999f38a",
   "metadata": {},
   "source": [
    "## Specifying hyperparameters and tuning them\n",
    "\n",
    "We first demonstrate hyperparameter-tuning and how you can provide your own validation dataset that AutoGluon internally relies on to: tune hyperparameters, early-stop iterative training, and construct model ensembles. One reason you may specify validation data is when future test data will stem from a different distribution than training data (and your specified validation data is more representative of the future data that will likely be encountered).\n",
    "\n",
    "If you don’t have a strong reason to provide your own validation dataset, we recommend you omit the `tuning_data` argument. This lets AutoGluon automatically select validation data from your provided training set (it uses smart strategies such as stratified sampling). For greater control, you can specify the `holdout_fra`c argument to tell AutoGluon what fraction of the provided training data to hold out for validation.\n",
    "\n",
    "**Caution:** Since AutoGluon tunes internal knobs based on this validation data, performance estimates reported on this data may be over-optimistic. For unbiased performance estimates, you should always call `predict()` on a separate dataset (that was never passed to `fit()`), as we did in the previous Quick-Start tutorial. We also emphasize that most options specified in this tutorial are chosen to minimize runtime for the purposes of demonstration and you should select more reasonable values in order to obtain high-quality models.\n",
    "\n",
    "`fit()` trains neural networks and various types of tree ensembles by default. You can specify various hyperparameter values for each type of model. For each hyperparameter, you can either specify a single fixed value, or a search space of values to consider during hyperparameter optimization. Hyperparameters which you do not specify are left at default settings chosen automatically by AutoGluon, which may be fixed values or search spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a49bd103",
   "metadata": {},
   "outputs": [],
   "source": [
    "import autogluon.core as ag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fe4d2f9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_epochs': 10,\n",
       " 'learning_rate': Real: lower=0.0001, upper=0.01,\n",
       " 'activation': Categorical['relu', 'softrelu', 'tanh'],\n",
       " 'layers': Categorical[[100], [1000], [200, 100], [300, 200, 100]],\n",
       " 'dropout_prob': Real: lower=0.0, upper=0.5}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# specifies non-default hyperparameter values for neural network models\n",
    "#    num_epochs: number of training epochs (controls training time of NN models)\n",
    "#    learning_rate: learning rate used in training (real-valued hyperparameter searched on log-scale)\n",
    "#    activation: activation function used in NN (categorical hyperparameter, default = first entry)\n",
    "#    layers: each choice for categorical hyperparameter 'layers' corresponds to list of sizes for each NN layer to use\n",
    "#    dropout_prob: dropout probability (real-valued hyperparameter)\n",
    "nn_options = {\n",
    "    'num_epochs': 10,\n",
    "    'learning_rate': ag.space.Real(1e-4, 1e-2, default=5e-4, log=True),\n",
    "    'activation': ag.space.Categorical('relu', 'softrelu', 'tanh'),\n",
    "    'layers': ag.space.Categorical([100], [1000], [200, 100], [300, 200, 100]),\n",
    "    'dropout_prob': ag.space.Real(0.0, 0.5, default=0.1)\n",
    "}\n",
    "\n",
    "nn_options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "10b6ea1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_boost_round': 100, 'num_leaves': Int: lower=26, upper=66}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# specifies non-default hyperparameter values for lightGBM gradient boosted trees\n",
    "#    num_boost_round: number of boosting rounds (controls training time of GBM models)\n",
    "#    num_leaves: number of leaves in trees (integer hyperparameter)\n",
    "gbm_options = {  \n",
    "    'num_boost_round': 100,\n",
    "    'num_leaves': ag.space.Int(lower=26, upper=66, default=36)\n",
    "}\n",
    "\n",
    "gbm_options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ee488e95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'GBM': {'num_boost_round': 100, 'num_leaves': Int: lower=26, upper=66},\n",
       " 'NN': {'num_epochs': 10,\n",
       "  'learning_rate': Real: lower=0.0001, upper=0.01,\n",
       "  'activation': Categorical['relu', 'softrelu', 'tanh'],\n",
       "  'layers': Categorical[[100], [1000], [200, 100], [300, 200, 100]],\n",
       "  'dropout_prob': Real: lower=0.0, upper=0.5}}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hyperparameters of each model type\n",
    "# When these keys are missing from hyperparameters dict, no models of that type are trained\n",
    "#    NN: NOTE: comment this line out if you get errors on Mac OSX\n",
    "hyperparameters = {  \n",
    "    'GBM': gbm_options,\n",
    "    'NN': nn_options\n",
    "}\n",
    "\n",
    "hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "be07c04a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3600"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train various models for ~2 min\n",
    "# time_limit = 2*60\n",
    "\n",
    "# https://www.kaggle.com/code/daikikatsuragawa/tps-mar-2021-benchmark-using-autogluon/notebook\n",
    "# If the value of time_limit is too small, we will get the following error:\n",
    "# ValueError: AutoGluon did not successfully train any models\n",
    "\n",
    "time_limit = 60*60\n",
    "time_limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "116958eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try at most 5 different hyperparameter configurations for each type of model\n",
    "num_trials = 5\n",
    "num_trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0fcc3549",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'auto'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to tune hyperparameters using Bayesian optimization routine with a local scheduler\n",
    "search_strategy = 'auto'\n",
    "search_strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "40c573b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_trials': 5, 'searcher': 'auto'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# HPO is not performed unless hyperparameter_tune_kwargs is specified\n",
    "hyperparameter_tune_kwargs = {\n",
    "    'num_trials': num_trials,\n",
    "    'searcher': search_strategy\n",
    "}\n",
    "\n",
    "hyperparameter_tune_kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d25603ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'target'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4b1b854c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'roc_auc'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c15c930b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\npredictor = TabularPredictor(label=label, eval_metric=metric).fit(\\n    train_data,\\n    tuning_data=val_data,\\n    time_limit=time_limit,\\n    hyperparameters=hyperparameters,\\n    hyperparameter_tune_kwargs=hyperparameter_tune_kwargs\\n)\\n'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NEED TO DEBUG\n",
    "\"\"\"\n",
    "predictor = TabularPredictor(label=label, eval_metric=metric).fit(\n",
    "    train_data,\n",
    "    tuning_data=val_data,\n",
    "    time_limit=time_limit,\n",
    "    hyperparameters=hyperparameters,\n",
    "    hyperparameter_tune_kwargs=hyperparameter_tune_kwargs\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c86016f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20230702_040607/\"\n",
      "Beginning AutoGluon training ... Time limit = 3600s\n",
      "AutoGluon will save models to \"AutogluonModels/ag-20230702_040607/\"\n",
      "AutoGluon Version:  0.7.0\n",
      "Python Version:     3.9.11\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Thu May 4 15:21:22 UTC 2023\n",
      "Train Data Rows:    476170\n",
      "Train Data Columns: 57\n",
      "Label Column: target\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'binary' (because only two unique label-values observed).\n",
      "\t2 unique label values:  [0, 1]\n",
      "\tIf 'binary' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    168131.98 MB\n",
      "\tTrain Data (Original)  Memory Usage: 170.47 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 18 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('category', []) : 14 | ['ps_ind_02_cat', 'ps_ind_04_cat', 'ps_ind_05_cat', 'ps_car_01_cat', 'ps_car_02_cat', ...]\n",
      "\t\t('float', [])    : 11 | ['ps_reg_01', 'ps_reg_02', 'ps_reg_03', 'ps_car_11', 'ps_car_12', ...]\n",
      "\t\t('int', [])      : 32 | ['ps_ind_01', 'ps_ind_03', 'ps_ind_06_bin', 'ps_ind_07_bin', 'ps_ind_08_bin', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])  : 13 | ['ps_ind_02_cat', 'ps_ind_04_cat', 'ps_ind_05_cat', 'ps_car_01_cat', 'ps_car_02_cat', ...]\n",
      "\t\t('float', [])     : 11 | ['ps_reg_01', 'ps_reg_02', 'ps_reg_03', 'ps_car_11', 'ps_car_12', ...]\n",
      "\t\t('int', [])       : 15 | ['ps_ind_01', 'ps_ind_03', 'ps_ind_14', 'ps_ind_15', 'ps_calc_04', ...]\n",
      "\t\t('int', ['bool']) : 18 | ['ps_ind_06_bin', 'ps_ind_07_bin', 'ps_ind_08_bin', 'ps_ind_09_bin', 'ps_ind_10_bin', ...]\n",
      "\t2.5s = Fit runtime\n",
      "\t57 features in original data used to generate 57 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 113.81 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 3.13s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'roc_auc'\n",
      "\tThis metric expects predicted probabilities rather than predicted class labels, so you'll need to use predict_proba() instead of predict()\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.01, Train Rows: 471408, Val Rows: 4762\n",
      "Fitting 13 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 3596.87s of the 3596.87s of remaining time.\n",
      "\t0.5039\t = Validation score   (roc_auc)\n",
      "\t3.27s\t = Training   runtime\n",
      "\t0.7s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 3592.63s of the 3592.62s of remaining time.\n",
      "\t0.5047\t = Validation score   (roc_auc)\n",
      "\t0.54s\t = Training   runtime\n",
      "\t0.64s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3591.14s of the 3591.14s of remaining time.\n",
      "\t0.5919\t = Validation score   (roc_auc)\n",
      "\t4.0s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 3587.03s of the 3587.03s of remaining time.\n",
      "\t0.5818\t = Validation score   (roc_auc)\n",
      "\t1.83s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ... Training model for up to 3585.11s of the 3585.1s of remaining time.\n",
      "\t0.583\t = Validation score   (roc_auc)\n",
      "\t15.18s\t = Training   runtime\n",
      "\t0.27s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ... Training model for up to 3566.95s of the 3566.95s of remaining time.\n",
      "\t0.5819\t = Validation score   (roc_auc)\n",
      "\t15.36s\t = Training   runtime\n",
      "\t0.16s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 3549.42s of the 3549.42s of remaining time.\n",
      "\t0.602\t = Validation score   (roc_auc)\n",
      "\t47.65s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini ... Training model for up to 3501.59s of the 3501.59s of remaining time.\n",
      "\t0.5851\t = Validation score   (roc_auc)\n",
      "\t9.53s\t = Training   runtime\n",
      "\t0.16s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr ... Training model for up to 3489.9s of the 3489.9s of remaining time.\n",
      "\t0.6018\t = Validation score   (roc_auc)\n",
      "\t9.6s\t = Training   runtime\n",
      "\t0.17s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 3478.12s of the 3478.12s of remaining time.\n",
      "No improvement since epoch 6: early stopping\n",
      "\t0.5908\t = Validation score   (roc_auc)\n",
      "\t302.46s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 3175.21s of the 3175.21s of remaining time.\n",
      "\t0.6015\t = Validation score   (roc_auc)\n",
      "\t7.18s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 3167.82s of the 3167.82s of remaining time.\n",
      "\t0.5942\t = Validation score   (roc_auc)\n",
      "\t237.41s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 2930.26s of the 2930.26s of remaining time.\n",
      "\t0.5871\t = Validation score   (roc_auc)\n",
      "\t2.92s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.0s of the 2927.17s of remaining time.\n",
      "\t0.6123\t = Validation score   (roc_auc)\n",
      "\t1.26s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 675.17s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20230702_040607/\")\n"
     ]
    }
   ],
   "source": [
    "predictor = TabularPredictor(label=label, eval_metric=metric).fit(\n",
    "    train_data,\n",
    "    time_limit=time_limit\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39fb9ebb",
   "metadata": {},
   "source": [
    "We again demonstrate how to use the trained models to predict on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f5f30be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred = predictor.predict(test_data_nolabel)\n",
    "# AssertionError: `evaluate_predictions` requires y_pred_proba input when evaluating \"roc_auc\"... \n",
    "# Please generate valid input via `predictor.predict_proba(data)`.\n",
    "y_pred = predictor.predict_proba(test_data_nolabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1e027396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions:\n",
      "           0         1\n",
      "0  0.952504  0.047496\n",
      "1  0.955137  0.044863\n",
      "2  0.972644  0.027356\n",
      "3  0.982837  0.017163\n",
      "4  0.978452  0.021548\n"
     ]
    }
   ],
   "source": [
    "# print(\"Predictions:  \", list(y_pred)[:5])\n",
    "print(\"Predictions:\\n\", y_pred[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3fa3c8be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: roc_auc on test data: 0.6318303042376591\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"roc_auc\": 0.6318303042376591\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "perf = predictor.evaluate_predictions(y_true=y_test, y_pred=y_pred, auxiliary_metrics=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ae2d78",
   "metadata": {},
   "source": [
    "Use the following to view a summary of what happened during fit. Now this command will show details of the hyperparameter-tuning process for each type of model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e442b79b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Summary of fit() ***\n",
      "Estimated performance of each model:\n",
      "                  model  score_val  pred_time_val    fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0   WeightedEnsemble_L2   0.612350       0.352664  605.562781                0.001182           1.260499            2       True         14\n",
      "1              CatBoost   0.602004       0.024931   47.651210                0.024931          47.651210            1       True          7\n",
      "2        ExtraTreesEntr   0.601840       0.167636    9.599753                0.167636           9.599753            1       True          9\n",
      "3               XGBoost   0.601470       0.037694    7.182353                0.037694           7.182353            1       True         11\n",
      "4        NeuralNetTorch   0.594228       0.057916  237.406210                0.057916         237.406210            1       True         12\n",
      "5            LightGBMXT   0.591881       0.014627    3.995357                0.014627           3.995357            1       True          3\n",
      "6       NeuralNetFastAI   0.590777       0.063305  302.462755                0.063305         302.462755            1       True         10\n",
      "7         LightGBMLarge   0.587142       0.013781    2.921548                0.013781           2.921548            1       True         13\n",
      "8        ExtraTreesGini   0.585093       0.161914    9.527112                0.161914           9.527112            1       True          8\n",
      "9      RandomForestGini   0.582968       0.270376   15.182387                0.270376          15.182387            1       True          5\n",
      "10     RandomForestEntr   0.581938       0.163673   15.362238                0.163673          15.362238            1       True          6\n",
      "11             LightGBM   0.581836       0.013024    1.831353                0.013024           1.831353            1       True          4\n",
      "12       KNeighborsDist   0.504697       0.644846    0.536084                0.644846           0.536084            1       True          2\n",
      "13       KNeighborsUnif   0.503897       0.699020    3.271789                0.699020           3.271789            1       True          1\n",
      "Number of models trained: 14\n",
      "Types of models trained:\n",
      "{'XGBoostModel', 'RFModel', 'WeightedEnsembleModel', 'KNNModel', 'NNFastAiTabularModel', 'LGBModel', 'XTModel', 'TabularNeuralNetTorchModel', 'CatBoostModel'}\n",
      "Bagging used: False \n",
      "Multi-layer stack-ensembling used: False \n",
      "Feature Metadata (Processed):\n",
      "(raw dtype, special dtypes):\n",
      "('category', [])  : 13 | ['ps_ind_02_cat', 'ps_ind_04_cat', 'ps_ind_05_cat', 'ps_car_01_cat', 'ps_car_02_cat', ...]\n",
      "('float', [])     : 11 | ['ps_reg_01', 'ps_reg_02', 'ps_reg_03', 'ps_car_11', 'ps_car_12', ...]\n",
      "('int', [])       : 15 | ['ps_ind_01', 'ps_ind_03', 'ps_ind_14', 'ps_ind_15', 'ps_calc_04', ...]\n",
      "('int', ['bool']) : 18 | ['ps_ind_06_bin', 'ps_ind_07_bin', 'ps_ind_08_bin', 'ps_ind_09_bin', 'ps_ind_10_bin', ...]\n",
      "Plot summary of models saved to file: AutogluonModels/ag-20230702_040607/SummaryOfModels.html\n",
      "*** End of fit() summary ***\n"
     ]
    }
   ],
   "source": [
    "results = predictor.fit_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58082bf8",
   "metadata": {},
   "source": [
    "In the above example, the predictive performance may be poor because we specified very little training to ensure quick runtimes. You can call `fit()` multiple times while modifying the above settings to better understand how these choices affect performance outcomes. For example: you can comment out the `train_data.head` command or increase `subsample_size` to train using a larger dataset, increase the `num_epochs` and `num_boost_round` hyperparameters, and increase the `time_limit` (which you should do for all code in these tutorials). To see more detailed output during the execution of `fit()`, you can also pass in the argument: `verbosity=3`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75bbc03",
   "metadata": {},
   "source": [
    "## Model ensembling with stacking/bagging\n",
    "\n",
    "Beyond hyperparameter-tuning with a correctly-specified evaluation metric, two other methods to boost predictive performance are bagging and stack-ensembling:\n",
    "\n",
    "\"AutoGluon-Tabular: Robust and Accurate AutoML for Structured Data\": https://arxiv.org/abs/2003.06505\n",
    "\n",
    "You’ll often see performance improve if you specify `num_bag_folds = 5-10, num_stack_levels = 1-3` in the call to `fit()`, but this will increase training times and memory/disk usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "945a2aef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20230702_041736/\"\n",
      "Warning: Training may take a very long time because `time_limit` was not specified and `train_data` is large (476170 samples, 170.95 MB).\n",
      "\tConsider setting `time_limit` to ensure training finishes within an expected duration or experiment with a small portion of `train_data` to identify an ideal `presets` and `hyperparameters` configuration.\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"AutogluonModels/ag-20230702_041736/\"\n",
      "AutoGluon Version:  0.7.0\n",
      "Python Version:     3.9.11\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Thu May 4 15:21:22 UTC 2023\n",
      "Train Data Rows:    476170\n",
      "Train Data Columns: 57\n",
      "Label Column: target\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'binary' (because only two unique label-values observed).\n",
      "\t2 unique label values:  [0, 1]\n",
      "\tIf 'binary' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    159806.81 MB\n",
      "\tTrain Data (Original)  Memory Usage: 170.47 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 18 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('category', []) : 14 | ['ps_ind_02_cat', 'ps_ind_04_cat', 'ps_ind_05_cat', 'ps_car_01_cat', 'ps_car_02_cat', ...]\n",
      "\t\t('float', [])    : 11 | ['ps_reg_01', 'ps_reg_02', 'ps_reg_03', 'ps_car_11', 'ps_car_12', ...]\n",
      "\t\t('int', [])      : 32 | ['ps_ind_01', 'ps_ind_03', 'ps_ind_06_bin', 'ps_ind_07_bin', 'ps_ind_08_bin', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])  : 13 | ['ps_ind_02_cat', 'ps_ind_04_cat', 'ps_ind_05_cat', 'ps_car_01_cat', 'ps_car_02_cat', ...]\n",
      "\t\t('float', [])     : 11 | ['ps_reg_01', 'ps_reg_02', 'ps_reg_03', 'ps_car_11', 'ps_car_12', ...]\n",
      "\t\t('int', [])       : 15 | ['ps_ind_01', 'ps_ind_03', 'ps_ind_14', 'ps_ind_15', 'ps_calc_04', ...]\n",
      "\t\t('int', ['bool']) : 18 | ['ps_ind_06_bin', 'ps_ind_07_bin', 'ps_ind_08_bin', 'ps_ind_09_bin', 'ps_ind_10_bin', ...]\n",
      "\t2.5s = Fit runtime\n",
      "\t57 features in original data used to generate 57 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 113.81 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 3.09s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'roc_auc'\n",
      "\tThis metric expects predicted probabilities rather than predicted class labels, so you'll need to use predict_proba() instead of predict()\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Fitting 13 L1 models ...\n",
      "Fitting model: KNeighborsUnif_BAG_L1 ...\n",
      "\t0.5077\t = Validation score   (roc_auc)\n",
      "\t0.29s\t = Training   runtime\n",
      "\t28.66s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist_BAG_L1 ...\n",
      "\t0.5076\t = Validation score   (roc_auc)\n",
      "\t0.27s\t = Training   runtime\n",
      "\t28.78s\t = Validation runtime\n",
      "Fitting model: LightGBMXT_BAG_L1 ...\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t0.632\t = Validation score   (roc_auc)\n",
      "\t6.24s\t = Training   runtime\n",
      "\t1.0s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ...\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t0.6294\t = Validation score   (roc_auc)\n",
      "\t5.26s\t = Training   runtime\n",
      "\t0.95s\t = Validation runtime\n",
      "Fitting model: RandomForestGini_BAG_L1 ...\n",
      "\t0.5995\t = Validation score   (roc_auc)\n",
      "\t15.22s\t = Training   runtime\n",
      "\t30.68s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr_BAG_L1 ...\n",
      "\t0.6033\t = Validation score   (roc_auc)\n",
      "\t15.11s\t = Training   runtime\n",
      "\t28.35s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ...\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t0.6411\t = Validation score   (roc_auc)\n",
      "\t288.4s\t = Training   runtime\n",
      "\t0.88s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini_BAG_L1 ...\n",
      "\t0.6019\t = Validation score   (roc_auc)\n",
      "\t9.43s\t = Training   runtime\n",
      "\t26.59s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr_BAG_L1 ...\n",
      "\t0.6028\t = Validation score   (roc_auc)\n",
      "\t9.72s\t = Training   runtime\n",
      "\t26.94s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L1 ...\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t0.6309\t = Validation score   (roc_auc)\n",
      "\t370.83s\t = Training   runtime\n",
      "\t5.05s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L1 ...\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t0.6389\t = Validation score   (roc_auc)\n",
      "\t10.7s\t = Training   runtime\n",
      "\t3.87s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L1 ...\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t0.6209\t = Validation score   (roc_auc)\n",
      "\t266.97s\t = Training   runtime\n",
      "\t4.99s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L1 ...\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t0.6292\t = Validation score   (roc_auc)\n",
      "\t7.72s\t = Training   runtime\n",
      "\t1.04s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\t0.6438\t = Validation score   (roc_auc)\n",
      "\t110.23s\t = Training   runtime\n",
      "\t0.09s\t = Validation runtime\n",
      "Fitting 11 L2 models ...\n",
      "Fitting model: LightGBMXT_BAG_L2 ...\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t0.6383\t = Validation score   (roc_auc)\n",
      "\t5.5s\t = Training   runtime\n",
      "\t0.8s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L2 ...\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t0.638\t = Validation score   (roc_auc)\n",
      "\t4.92s\t = Training   runtime\n",
      "\t0.73s\t = Validation runtime\n",
      "Fitting model: RandomForestGini_BAG_L2 ...\n",
      "\t0.6009\t = Validation score   (roc_auc)\n",
      "\t27.17s\t = Training   runtime\n",
      "\t34.52s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr_BAG_L2 ...\n",
      "\t0.604\t = Validation score   (roc_auc)\n",
      "\t32.62s\t = Training   runtime\n",
      "\t33.86s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L2 ...\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t0.6427\t = Validation score   (roc_auc)\n",
      "\t64.76s\t = Training   runtime\n",
      "\t0.8s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini_BAG_L2 ...\n",
      "\t0.6132\t = Validation score   (roc_auc)\n",
      "\t11.12s\t = Training   runtime\n",
      "\t32.72s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr_BAG_L2 ...\n",
      "\t0.6139\t = Validation score   (roc_auc)\n",
      "\t11.56s\t = Training   runtime\n",
      "\t32.89s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L2 ...\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t0.638\t = Validation score   (roc_auc)\n",
      "\t361.77s\t = Training   runtime\n",
      "\t4.93s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L2 ...\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t0.6392\t = Validation score   (roc_auc)\n",
      "\t10.31s\t = Training   runtime\n",
      "\t4.14s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L2 ...\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t0.6328\t = Validation score   (roc_auc)\n",
      "\t246.41s\t = Training   runtime\n",
      "\t6.25s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L2 ...\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t0.6361\t = Validation score   (roc_auc)\n",
      "\t7.58s\t = Training   runtime\n",
      "\t0.8s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L3 ...\n",
      "\t0.643\t = Validation score   (roc_auc)\n",
      "\t93.91s\t = Training   runtime\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t0.09s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 2433.87s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20230702_041736/\")\n"
     ]
    }
   ],
   "source": [
    "# last  argument is just for quick demo here, omit it in real applications\n",
    "predictor = TabularPredictor(label=label, eval_metric=metric).fit(train_data,\n",
    "    num_bag_folds=5, num_bag_sets=1, num_stack_levels=1,\n",
    "    # num_bag_folds=5, num_bag_sets=1, num_stack_levels=3,                                                              \n",
    "    # hyperparameters = {'NN': {'num_epochs': 2}, 'GBM': {'num_boost_round': 20}},  \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c6e1bd",
   "metadata": {},
   "source": [
    "You should not provide `tuning_data` when stacking/bagging, and instead provide all your available data as `train_data` (which AutoGluon will split in more intellgent ways). `num_bag_sets` controls how many times the k-fold bagging process is repeated to further reduce variance (increasing this may further boost accuracy but will substantially increase training times, inference latency, and memory/disk usage). Rather than manually searching for good bagging/stacking values yourself, AutoGluon will automatically select good values for you if you specify `auto_stack` instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4178bef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stack configuration (auto_stack=True): num_stack_levels=0, num_bag_folds=8, num_bag_sets=20\n",
      "Beginning AutoGluon training ... Time limit = 30s\n",
      "AutoGluon will save models to \"agModels-predict_porto_2/\"\n",
      "AutoGluon Version:  0.7.0\n",
      "Python Version:     3.9.11\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Thu May 4 15:21:22 UTC 2023\n",
      "Train Data Rows:    476170\n",
      "Train Data Columns: 57\n",
      "Label Column: target\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'binary' (because only two unique label-values observed).\n",
      "\t2 unique label values:  [0, 1]\n",
      "\tIf 'binary' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    154564.05 MB\n",
      "\tTrain Data (Original)  Memory Usage: 170.47 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 18 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('category', []) : 14 | ['ps_ind_02_cat', 'ps_ind_04_cat', 'ps_ind_05_cat', 'ps_car_01_cat', 'ps_car_02_cat', ...]\n",
      "\t\t('float', [])    : 11 | ['ps_reg_01', 'ps_reg_02', 'ps_reg_03', 'ps_car_11', 'ps_car_12', ...]\n",
      "\t\t('int', [])      : 32 | ['ps_ind_01', 'ps_ind_03', 'ps_ind_06_bin', 'ps_ind_07_bin', 'ps_ind_08_bin', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])  : 13 | ['ps_ind_02_cat', 'ps_ind_04_cat', 'ps_ind_05_cat', 'ps_car_01_cat', 'ps_car_02_cat', ...]\n",
      "\t\t('float', [])     : 11 | ['ps_reg_01', 'ps_reg_02', 'ps_reg_03', 'ps_car_11', 'ps_car_12', ...]\n",
      "\t\t('int', [])       : 15 | ['ps_ind_01', 'ps_ind_03', 'ps_ind_14', 'ps_ind_15', 'ps_calc_04', ...]\n",
      "\t\t('int', ['bool']) : 18 | ['ps_ind_06_bin', 'ps_ind_07_bin', 'ps_ind_08_bin', 'ps_ind_09_bin', 'ps_ind_10_bin', ...]\n",
      "\t2.5s = Fit runtime\n",
      "\t57 features in original data used to generate 57 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 113.81 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 3.14s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'roc_auc'\n",
      "\tThis metric expects predicted probabilities rather than predicted class labels, so you'll need to use predict_proba() instead of predict()\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "\tWARNING: \"NN\" model has been deprecated in v0.4.0 and renamed to \"NN_MXNET\". Starting in v0.6.0, specifying \"NN\" or \"NN_MXNET\" will raise an exception. Consider instead specifying \"NN_TORCH\".\n",
      "Fitting 2 L1 models ...\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 26.86s of the 26.86s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t0.624\t = Validation score   (roc_auc)\n",
      "\t4.02s\t = Training   runtime\n",
      "\t0.74s\t = Validation runtime\n",
      "Fitting model: NeuralNetMXNet_BAG_L1 ... Training model for up to 16.09s of the 16.09s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused NeuralNetMXNet_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=65466, ip=10.164.150.180)\n",
      "ModuleNotFoundError: No module named 'mxnet'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "\u001b[36mray::_ray_fit()\u001b[39m (pid=65466, ip=10.164.150.180)\n",
      "  File \"/home/stever7/.local/lib/python3.9/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 374, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold,\n",
      "  File \"/home/stever7/.local/lib/python3.9/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 703, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"/home/stever7/.local/lib/python3.9/site-packages/autogluon/tabular/models/tabular_nn/mxnet/tabular_nn_mxnet.py\", line 135, in _fit\n",
      "    try_import_mxnet()\n",
      "  File \"/home/stever7/.local/lib/python3.9/site-packages/autogluon/core/utils/try_import.py\", line 49, in try_import_mxnet\n",
      "    raise ImportError(\n",
      "ImportError: Unable to import dependency mxnet. A quick tip is to install via `pip install mxnet --upgrade`, or `pip install mxnet_cu101 --upgrade`\n",
      "Completed 1/20 k-fold bagging repeats ...\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 26.86s of the 9.89s of remaining time.\n",
      "\t0.624\t = Validation score   (roc_auc)\n",
      "\t0.09s\t = Training   runtime\n",
      "\t0.09s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 20.9s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"agModels-predict_porto_2/\")\n"
     ]
    }
   ],
   "source": [
    "save_path = 'agModels-predict_porto_2'  # folder where to store trained models\n",
    "\n",
    "# last 2 arguments are for quick demo, omit them in real applications\n",
    "predictor = TabularPredictor(label=label, eval_metric=metric, path=save_path).fit(\n",
    "    train_data, \n",
    "    auto_stack=True,\n",
    "    time_limit=30, \n",
    "    hyperparameters={'NN': {'num_epochs': 2}, 'GBM': {'num_boost_round': 20}} \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a29c64",
   "metadata": {},
   "source": [
    "Often stacking/bagging will produce superior accuracy than hyperparameter-tuning, but you may try combining both techniques (note: specifying `presets='best_quality'` in `fit()` simply sets `auto_stack=True`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f098bce8",
   "metadata": {},
   "source": [
    "## Prediction options (inference)\n",
    "\n",
    "Even if you’ve started a new Python session since last calling `fit()`, you can still load a previously trained predictor from disk:\n",
    "\n",
    "```\n",
    "# `predictor.path` is another way to get the relative path needed to later load predictor.\n",
    "predictor = TabularPredictor.load(save_path)  \n",
    "```\n",
    "\n",
    "Above `save_path` is the same folder previously passed to `TabularPredictor`, in which all the trained models have been saved. You can train easily models on one machine and deploy them on another. Simply copy the `save_path` folder to the new machine and specify its new path in `TabularPredictor.load()`.\n",
    "\n",
    "We can make a prediction on an individual example rather than a full dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3cf009e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-02 04:58:31,848\tERROR worker.py:400 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2023-07-02 04:58:31,849\tERROR worker.py:400 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2023-07-02 04:58:31,853\tERROR worker.py:400 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2023-07-02 04:58:31,854\tERROR worker.py:400 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2023-07-02 04:58:31,856\tERROR worker.py:400 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2023-07-02 04:58:31,856\tERROR worker.py:400 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2023-07-02 04:58:31,857\tERROR worker.py:400 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ps_ind_01 ps_ind_02_cat  ps_ind_03 ps_ind_04_cat ps_ind_05_cat  \\\n",
      "0          2           2.0          5           1.0           0.0   \n",
      "\n",
      "   ps_ind_06_bin  ps_ind_07_bin  ps_ind_08_bin  ps_ind_09_bin  ps_ind_10_bin  \\\n",
      "0              0              1              0              0              0   \n",
      "\n",
      "   ...  ps_calc_11  ps_calc_12  ps_calc_13  ps_calc_14  ps_calc_15_bin  \\\n",
      "0  ...           9           1           5           8               0   \n",
      "\n",
      "   ps_calc_16_bin  ps_calc_17_bin  ps_calc_18_bin  ps_calc_19_bin  \\\n",
      "0               1               1               0               0   \n",
      "\n",
      "   ps_calc_20_bin  \n",
      "0               1  \n",
      "\n",
      "[1 rows x 57 columns]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "Name: target, dtype: object"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datapoint = test_data_nolabel.iloc[[0]]  # Note: .iloc[0] won't work because it returns pandas Series instead of DataFrame\n",
    "\n",
    "print(datapoint)\n",
    "predictor.predict(datapoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12589a0",
   "metadata": {},
   "source": [
    "To output predicted class probabilities instead of predicted classes, you can use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8f24d4e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.956901</td>\n",
       "      <td>0.043099</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1\n",
       "0  0.956901  0.043099"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.predict_proba(datapoint)  # returns a DataFrame that shows which probability corresponds to which class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5fa765",
   "metadata": {},
   "source": [
    "By default, `predict()` and `predict_proba()` will utilize the model that AutoGluon thinks is most accurate, which is usually an ensemble of many individual models. Here’s how to see which model this is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8540150a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'WeightedEnsemble_L2'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.get_model_best()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a2f3ee",
   "metadata": {},
   "source": [
    "We can instead specify a particular model to use for predictions (e.g. to reduce inference latency). Note that a ‘model’ in AutoGluon may refer to for example a single Neural Network, a bagged ensemble of many Neural Network copies trained on different training/validation splits, a weighted ensemble that aggregates the predictions of many other models, or a stacker model that operates on predictions output by other models. This is akin to viewing a Random Forest as one ‘model’ when it is in fact an ensemble of many decision trees.\n",
    "\n",
    "Before deciding which model to use, let’s evaluate all of the models AutoGluon has previously trained on our test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3cd76691",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <td>LightGBM_BAG_L1</td>\n",
       "      <td>WeightedEnsemble_L2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>score_test</th>\n",
       "      <td>0.618328</td>\n",
       "      <td>0.618328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>score_val</th>\n",
       "      <td>0.623974</td>\n",
       "      <td>0.623974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pred_time_test</th>\n",
       "      <td>0.762287</td>\n",
       "      <td>0.767083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pred_time_val</th>\n",
       "      <td>0.744501</td>\n",
       "      <td>0.834036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fit_time</th>\n",
       "      <td>4.017492</td>\n",
       "      <td>4.108663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pred_time_test_marginal</th>\n",
       "      <td>0.762287</td>\n",
       "      <td>0.004795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pred_time_val_marginal</th>\n",
       "      <td>0.744501</td>\n",
       "      <td>0.089535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fit_time_marginal</th>\n",
       "      <td>4.017492</td>\n",
       "      <td>0.091171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stack_level</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>can_infer</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fit_order</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       0                    1\n",
       "model                    LightGBM_BAG_L1  WeightedEnsemble_L2\n",
       "score_test                      0.618328             0.618328\n",
       "score_val                       0.623974             0.623974\n",
       "pred_time_test                  0.762287             0.767083\n",
       "pred_time_val                   0.744501             0.834036\n",
       "fit_time                        4.017492             4.108663\n",
       "pred_time_test_marginal         0.762287             0.004795\n",
       "pred_time_val_marginal          0.744501             0.089535\n",
       "fit_time_marginal               4.017492             0.091171\n",
       "stack_level                            1                    2\n",
       "can_infer                           True                 True\n",
       "fit_order                              1                    2"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predictor.leaderboard(test_data, silent=True)\n",
    "predictor.leaderboard(test_data, silent=True).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9b0075",
   "metadata": {},
   "source": [
    "The leaderboard shows each model’s predictive performance on the test data (`score_test`) and validation data (`score_val`), as well as the time required to: produce predictions for the test data (`pred_time_val`), produce predictions on the validation data (`pred_time_val`), and train only this model (`fit_time`). Below, we show that a leaderboard can be produced without new data (just uses the data previously reserved for validation inside `fit`) and can display extra information about each model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "86aca8b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <td>LightGBM_BAG_L1</td>\n",
       "      <td>WeightedEnsemble_L2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>score_val</th>\n",
       "      <td>0.623974</td>\n",
       "      <td>0.623974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pred_time_val</th>\n",
       "      <td>0.744501</td>\n",
       "      <td>0.834036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fit_time</th>\n",
       "      <td>4.017492</td>\n",
       "      <td>4.108663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pred_time_val_marginal</th>\n",
       "      <td>0.744501</td>\n",
       "      <td>0.089535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fit_time_marginal</th>\n",
       "      <td>4.017492</td>\n",
       "      <td>0.091171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stack_level</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>can_infer</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fit_order</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>num_features</th>\n",
       "      <td>57</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>num_models</th>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>num_models_w_ancestors</th>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>memory_size</th>\n",
       "      <td>681221</td>\n",
       "      <td>2748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>memory_size_w_ancestors</th>\n",
       "      <td>681221</td>\n",
       "      <td>683969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>memory_size_min</th>\n",
       "      <td>88920</td>\n",
       "      <td>2748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>memory_size_min_w_ancestors</th>\n",
       "      <td>88920</td>\n",
       "      <td>88920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>num_ancestors</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>num_descendants</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model_type</th>\n",
       "      <td>StackerEnsembleModel</td>\n",
       "      <td>WeightedEnsembleModel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>child_model_type</th>\n",
       "      <td>LGBModel</td>\n",
       "      <td>GreedyWeightedEnsembleModel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hyperparameters</th>\n",
       "      <td>{'use_orig_features': True, 'max_base_models': 25, 'max_base_models_per_type': 5, 'save_bag_folds': True}</td>\n",
       "      <td>{'use_orig_features': False, 'max_base_models': 25, 'max_base_models_per_type': 5, 'save_bag_folds': True}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hyperparameters_fit</th>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ag_args_fit</th>\n",
       "      <td>{'max_memory_usage_ratio': 1.0, 'max_time_limit_ratio': 1.0, 'max_time_limit': None, 'min_time_limit': 0, 'valid_raw_types': None, 'valid_special_types': None, 'ignored_type_group_special': None, 'ignored_type_group_raw': None, 'get_features_kwargs': None, 'get_features_kwargs_extra': None, 'predict_1_batch_size': None, 'temperature_scalar': None, 'drop_unique': False}</td>\n",
       "      <td>{'max_memory_usage_ratio': 1.0, 'max_time_limit_ratio': 1.0, 'max_time_limit': None, 'min_time_limit': 0, 'valid_raw_types': None, 'valid_special_types': None, 'ignored_type_group_special': None, 'ignored_type_group_raw': None, 'get_features_kwargs': None, 'get_features_kwargs_extra': None, 'predict_1_batch_size': None, 'temperature_scalar': None, 'drop_unique': False}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>features</th>\n",
       "      <td>[ps_calc_01, ps_calc_13, ps_car_05_cat, ps_car_08_cat, ps_car_10_cat, ps_ind_18_bin, ps_calc_05, ps_car_14, ps_car_06_cat, ps_reg_02, ps_ind_07_bin, ps_ind_08_bin, ps_car_04_cat, ps_ind_06_bin, ps_ind_12_bin, ps_ind_05_cat, ps_calc_20_bin, ps_car_15, ps_ind_13_bin, ps_ind_14, ps_car_13, ps_calc_14, ps_calc_10, ps_car_01_cat, ps_calc_16_bin, ps_car_03_cat, ps_calc_07, ps_calc_11, ps_ind_01, ps_reg_01, ps_calc_12, ps_calc_02, ps_calc_04, ps_ind_04_cat, ps_reg_03, ps_calc_19_bin, ps_car_09_cat, ps_calc_03, ps_car_02_cat, ps_calc_08, ps_calc_17_bin, ps_ind_16_bin, ps_car_07_cat, ps_ind_03, ps_...</td>\n",
       "      <td>[LightGBM_BAG_L1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>compile_time</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>child_hyperparameters</th>\n",
       "      <td>{'learning_rate': 0.05, 'num_boost_round': 20}</td>\n",
       "      <td>{'ensemble_size': 100}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>child_hyperparameters_fit</th>\n",
       "      <td>{'num_boost_round': 20}</td>\n",
       "      <td>{'ensemble_size': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>child_ag_args_fit</th>\n",
       "      <td>{'max_memory_usage_ratio': 1.0, 'max_time_limit_ratio': 1.0, 'max_time_limit': None, 'min_time_limit': 0, 'valid_raw_types': ['bool', 'int', 'float', 'category'], 'valid_special_types': None, 'ignored_type_group_special': None, 'ignored_type_group_raw': None, 'get_features_kwargs': None, 'get_features_kwargs_extra': None, 'predict_1_batch_size': None, 'temperature_scalar': None}</td>\n",
       "      <td>{'max_memory_usage_ratio': 1.0, 'max_time_limit_ratio': 1.0, 'max_time_limit': None, 'min_time_limit': 0, 'valid_raw_types': None, 'valid_special_types': None, 'ignored_type_group_special': None, 'ignored_type_group_raw': None, 'get_features_kwargs': None, 'get_features_kwargs_extra': None, 'predict_1_batch_size': None, 'temperature_scalar': None, 'drop_unique': False}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ancestors</th>\n",
       "      <td>[]</td>\n",
       "      <td>[LightGBM_BAG_L1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>descendants</th>\n",
       "      <td>[WeightedEnsemble_L2]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   0  \\\n",
       "model                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                LightGBM_BAG_L1   \n",
       "score_val                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   0.623974   \n",
       "pred_time_val                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               0.744501   \n",
       "fit_time                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    4.017492   \n",
       "pred_time_val_marginal                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      0.744501   \n",
       "fit_time_marginal                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           4.017492   \n",
       "stack_level                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        1   \n",
       "can_infer                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       True   \n",
       "fit_order                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          1   \n",
       "num_features                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      57   \n",
       "num_models                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         8   \n",
       "num_models_w_ancestors                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             8   \n",
       "memory_size                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   681221   \n",
       "memory_size_w_ancestors                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       681221   \n",
       "memory_size_min                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                88920   \n",
       "memory_size_min_w_ancestors                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    88920   \n",
       "num_ancestors                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      0   \n",
       "num_descendants                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    1   \n",
       "model_type                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      StackerEnsembleModel   \n",
       "child_model_type                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            LGBModel   \n",
       "hyperparameters                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            {'use_orig_features': True, 'max_base_models': 25, 'max_base_models_per_type': 5, 'save_bag_folds': True}   \n",
       "hyperparameters_fit                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               {}   \n",
       "ag_args_fit                                                                                                                                                                                                                                                      {'max_memory_usage_ratio': 1.0, 'max_time_limit_ratio': 1.0, 'max_time_limit': None, 'min_time_limit': 0, 'valid_raw_types': None, 'valid_special_types': None, 'ignored_type_group_special': None, 'ignored_type_group_raw': None, 'get_features_kwargs': None, 'get_features_kwargs_extra': None, 'predict_1_batch_size': None, 'temperature_scalar': None, 'drop_unique': False}   \n",
       "features                     [ps_calc_01, ps_calc_13, ps_car_05_cat, ps_car_08_cat, ps_car_10_cat, ps_ind_18_bin, ps_calc_05, ps_car_14, ps_car_06_cat, ps_reg_02, ps_ind_07_bin, ps_ind_08_bin, ps_car_04_cat, ps_ind_06_bin, ps_ind_12_bin, ps_ind_05_cat, ps_calc_20_bin, ps_car_15, ps_ind_13_bin, ps_ind_14, ps_car_13, ps_calc_14, ps_calc_10, ps_car_01_cat, ps_calc_16_bin, ps_car_03_cat, ps_calc_07, ps_calc_11, ps_ind_01, ps_reg_01, ps_calc_12, ps_calc_02, ps_calc_04, ps_ind_04_cat, ps_reg_03, ps_calc_19_bin, ps_car_09_cat, ps_calc_03, ps_car_02_cat, ps_calc_08, ps_calc_17_bin, ps_ind_16_bin, ps_car_07_cat, ps_ind_03, ps_...   \n",
       "compile_time                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    None   \n",
       "child_hyperparameters                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 {'learning_rate': 0.05, 'num_boost_round': 20}   \n",
       "child_hyperparameters_fit                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    {'num_boost_round': 20}   \n",
       "child_ag_args_fit                                                                                                                                                                                                                                      {'max_memory_usage_ratio': 1.0, 'max_time_limit_ratio': 1.0, 'max_time_limit': None, 'min_time_limit': 0, 'valid_raw_types': ['bool', 'int', 'float', 'category'], 'valid_special_types': None, 'ignored_type_group_special': None, 'ignored_type_group_raw': None, 'get_features_kwargs': None, 'get_features_kwargs_extra': None, 'predict_1_batch_size': None, 'temperature_scalar': None}   \n",
       "ancestors                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         []   \n",
       "descendants                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    [WeightedEnsemble_L2]   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                               1  \n",
       "model                                                                                                                                                                                                                                                                                                                                                                                        WeightedEnsemble_L2  \n",
       "score_val                                                                                                                                                                                                                                                                                                                                                                                               0.623974  \n",
       "pred_time_val                                                                                                                                                                                                                                                                                                                                                                                           0.834036  \n",
       "fit_time                                                                                                                                                                                                                                                                                                                                                                                                4.108663  \n",
       "pred_time_val_marginal                                                                                                                                                                                                                                                                                                                                                                                  0.089535  \n",
       "fit_time_marginal                                                                                                                                                                                                                                                                                                                                                                                       0.091171  \n",
       "stack_level                                                                                                                                                                                                                                                                                                                                                                                                    2  \n",
       "can_infer                                                                                                                                                                                                                                                                                                                                                                                                   True  \n",
       "fit_order                                                                                                                                                                                                                                                                                                                                                                                                      2  \n",
       "num_features                                                                                                                                                                                                                                                                                                                                                                                                   1  \n",
       "num_models                                                                                                                                                                                                                                                                                                                                                                                                     1  \n",
       "num_models_w_ancestors                                                                                                                                                                                                                                                                                                                                                                                         9  \n",
       "memory_size                                                                                                                                                                                                                                                                                                                                                                                                 2748  \n",
       "memory_size_w_ancestors                                                                                                                                                                                                                                                                                                                                                                                   683969  \n",
       "memory_size_min                                                                                                                                                                                                                                                                                                                                                                                             2748  \n",
       "memory_size_min_w_ancestors                                                                                                                                                                                                                                                                                                                                                                                88920  \n",
       "num_ancestors                                                                                                                                                                                                                                                                                                                                                                                                  1  \n",
       "num_descendants                                                                                                                                                                                                                                                                                                                                                                                                0  \n",
       "model_type                                                                                                                                                                                                                                                                                                                                                                                 WeightedEnsembleModel  \n",
       "child_model_type                                                                                                                                                                                                                                                                                                                                                                     GreedyWeightedEnsembleModel  \n",
       "hyperparameters                                                                                                                                                                                                                                                                                       {'use_orig_features': False, 'max_base_models': 25, 'max_base_models_per_type': 5, 'save_bag_folds': True}  \n",
       "hyperparameters_fit                                                                                                                                                                                                                                                                                                                                                                                           {}  \n",
       "ag_args_fit                  {'max_memory_usage_ratio': 1.0, 'max_time_limit_ratio': 1.0, 'max_time_limit': None, 'min_time_limit': 0, 'valid_raw_types': None, 'valid_special_types': None, 'ignored_type_group_special': None, 'ignored_type_group_raw': None, 'get_features_kwargs': None, 'get_features_kwargs_extra': None, 'predict_1_batch_size': None, 'temperature_scalar': None, 'drop_unique': False}  \n",
       "features                                                                                                                                                                                                                                                                                                                                                                                       [LightGBM_BAG_L1]  \n",
       "compile_time                                                                                                                                                                                                                                                                                                                                                                                                None  \n",
       "child_hyperparameters                                                                                                                                                                                                                                                                                                                                                                     {'ensemble_size': 100}  \n",
       "child_hyperparameters_fit                                                                                                                                                                                                                                                                                                                                                                   {'ensemble_size': 1}  \n",
       "child_ag_args_fit            {'max_memory_usage_ratio': 1.0, 'max_time_limit_ratio': 1.0, 'max_time_limit': None, 'min_time_limit': 0, 'valid_raw_types': None, 'valid_special_types': None, 'ignored_type_group_special': None, 'ignored_type_group_raw': None, 'get_features_kwargs': None, 'get_features_kwargs_extra': None, 'predict_1_batch_size': None, 'temperature_scalar': None, 'drop_unique': False}  \n",
       "ancestors                                                                                                                                                                                                                                                                                                                                                                                      [LightGBM_BAG_L1]  \n",
       "descendants                                                                                                                                                                                                                                                                                                                                                                                                   []  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predictor.leaderboard(extra_info=True, silent=True)\n",
    "predictor.leaderboard(extra_info=True, silent=True).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8ef0ee",
   "metadata": {},
   "source": [
    "The expanded leaderboard shows properties like how many features are used by each model (`num_features`), which other models are ancestors whose predictions are required inputs for each model (`ancestors`), and how much memory each model and all its ancestors would occupy if simultaneously persisted (`memory_size_w_ancestors`). See the leaderboard documentation for full details:\n",
    "\n",
    "https://auto.gluon.ai/0.1.0/api/autogluon.predictor.html#autogluon.tabular.TabularPredictor.leaderboard\n",
    "\n",
    "Here’s how to specify a particular model to use for prediction instead of AutoGluon’s default model-choice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bfe9ad25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction from LightGBM_BAG_L1 model: 0\n"
     ]
    }
   ],
   "source": [
    "i = 0  # index of model to use\n",
    "model_to_use = predictor.get_model_names()[i]\n",
    "model_pred = predictor.predict(datapoint, model=model_to_use)\n",
    "print(\"Prediction from %s model: %s\" % (model_to_use, model_pred.iloc[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c0dc59",
   "metadata": {},
   "source": [
    "We can easily access various information about the trained predictor or a particular model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "574af829",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_models = predictor.get_model_names()\n",
    "model_to_use = all_models[i]\n",
    "specific_model = predictor._trainer.load_model(model_to_use)\n",
    "\n",
    "# Objects defined below are dicts of various information (not printed here as they are quite large):\n",
    "model_info = specific_model.get_info()\n",
    "predictor_information = predictor.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97dbd302",
   "metadata": {},
   "source": [
    "The `predictor` also remembers what metric predictions should be evaluated with, which can be done with ground truth labels as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2615fc21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stever7/.local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Evaluation: roc_auc on test data: 0.6183275066932732\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"roc_auc\": 0.6183275066932732,\n",
      "    \"accuracy\": 0.9635842811780716,\n",
      "    \"balanced_accuracy\": 0.5,\n",
      "    \"mcc\": 0.0,\n",
      "    \"f1\": 0.0,\n",
      "    \"precision\": 0.0,\n",
      "    \"recall\": 0.0\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# y_pred = predictor.predict(test_data_nolabel)\n",
    "y_pred = predictor.predict_proba(test_data_nolabel)\n",
    "perf = predictor.evaluate_predictions(y_true=y_test, y_pred=y_pred, auxiliary_metrics=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47325a9c",
   "metadata": {},
   "source": [
    "However, you must be careful here as certain metrics require predicted probabilities rather than classes. Since the label columns remains in the `test_data` DataFrame, we can instead use the shorthand:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9b3bace2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stever7/.local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Evaluation: roc_auc on test data: 0.6183275066932732\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"roc_auc\": 0.6183275066932732,\n",
      "    \"accuracy\": 0.9635842811780716,\n",
      "    \"balanced_accuracy\": 0.5,\n",
      "    \"mcc\": 0.0,\n",
      "    \"f1\": 0.0,\n",
      "    \"precision\": 0.0,\n",
      "    \"recall\": 0.0\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "perf = predictor.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ddcc4d",
   "metadata": {},
   "source": [
    "which will correctly select between `predict()` or `predict_proba()` depending on the evaluation metric."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99659f8a",
   "metadata": {},
   "source": [
    "## Interpretability (feature importance)\n",
    "\n",
    "To better understand our trained predictor, we can estimate the overall importance of each feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9056d799",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing feature importance via permutation shuffling for 57 features using 5000 rows with 5 shuffle sets...\n",
      "\t41.09s\t= Expected runtime (8.22s per shuffle set)\n",
      "\t12.58s\t= Actual runtime (Completed 5 of 5 shuffle sets)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>importance</th>\n",
       "      <th>stddev</th>\n",
       "      <th>p_value</th>\n",
       "      <th>n</th>\n",
       "      <th>p99_high</th>\n",
       "      <th>p99_low</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ps_ind_05_cat</th>\n",
       "      <td>2.336078e-02</td>\n",
       "      <td>0.007582</td>\n",
       "      <td>0.001163</td>\n",
       "      <td>5</td>\n",
       "      <td>0.038971</td>\n",
       "      <td>0.007750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_car_13</th>\n",
       "      <td>7.689422e-03</td>\n",
       "      <td>0.011673</td>\n",
       "      <td>0.107378</td>\n",
       "      <td>5</td>\n",
       "      <td>0.031725</td>\n",
       "      <td>-0.016346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_reg_03</th>\n",
       "      <td>7.202950e-03</td>\n",
       "      <td>0.002470</td>\n",
       "      <td>0.001429</td>\n",
       "      <td>5</td>\n",
       "      <td>0.012289</td>\n",
       "      <td>0.002117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_car_07_cat</th>\n",
       "      <td>4.274511e-03</td>\n",
       "      <td>0.007997</td>\n",
       "      <td>0.149022</td>\n",
       "      <td>5</td>\n",
       "      <td>0.020741</td>\n",
       "      <td>-0.012192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_ind_03</th>\n",
       "      <td>3.842139e-03</td>\n",
       "      <td>0.002970</td>\n",
       "      <td>0.022226</td>\n",
       "      <td>5</td>\n",
       "      <td>0.009958</td>\n",
       "      <td>-0.002274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_ind_17_bin</th>\n",
       "      <td>3.671786e-03</td>\n",
       "      <td>0.004612</td>\n",
       "      <td>0.074835</td>\n",
       "      <td>5</td>\n",
       "      <td>0.013169</td>\n",
       "      <td>-0.005825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_reg_01</th>\n",
       "      <td>3.067102e-03</td>\n",
       "      <td>0.002393</td>\n",
       "      <td>0.022816</td>\n",
       "      <td>5</td>\n",
       "      <td>0.007993</td>\n",
       "      <td>-0.001859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_ind_15</th>\n",
       "      <td>2.409103e-03</td>\n",
       "      <td>0.001559</td>\n",
       "      <td>0.012969</td>\n",
       "      <td>5</td>\n",
       "      <td>0.005620</td>\n",
       "      <td>-0.000801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_car_01_cat</th>\n",
       "      <td>2.277008e-03</td>\n",
       "      <td>0.004534</td>\n",
       "      <td>0.162147</td>\n",
       "      <td>5</td>\n",
       "      <td>0.011613</td>\n",
       "      <td>-0.007059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_ind_06_bin</th>\n",
       "      <td>1.987626e-03</td>\n",
       "      <td>0.005054</td>\n",
       "      <td>0.214437</td>\n",
       "      <td>5</td>\n",
       "      <td>0.012395</td>\n",
       "      <td>-0.008419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_car_03_cat</th>\n",
       "      <td>1.599415e-03</td>\n",
       "      <td>0.002446</td>\n",
       "      <td>0.108797</td>\n",
       "      <td>5</td>\n",
       "      <td>0.006637</td>\n",
       "      <td>-0.003438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_ind_16_bin</th>\n",
       "      <td>1.404765e-03</td>\n",
       "      <td>0.001391</td>\n",
       "      <td>0.043453</td>\n",
       "      <td>5</td>\n",
       "      <td>0.004270</td>\n",
       "      <td>-0.001460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_reg_02</th>\n",
       "      <td>7.112528e-04</td>\n",
       "      <td>0.001528</td>\n",
       "      <td>0.178328</td>\n",
       "      <td>5</td>\n",
       "      <td>0.003857</td>\n",
       "      <td>-0.002434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_car_14</th>\n",
       "      <td>3.650170e-04</td>\n",
       "      <td>0.000281</td>\n",
       "      <td>0.022066</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000945</td>\n",
       "      <td>-0.000215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_ind_02_cat</th>\n",
       "      <td>2.597998e-04</td>\n",
       "      <td>0.000294</td>\n",
       "      <td>0.059689</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000865</td>\n",
       "      <td>-0.000346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_calc_03</th>\n",
       "      <td>1.792607e-04</td>\n",
       "      <td>0.000418</td>\n",
       "      <td>0.195858</td>\n",
       "      <td>5</td>\n",
       "      <td>0.001040</td>\n",
       "      <td>-0.000681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_car_15</th>\n",
       "      <td>1.097464e-04</td>\n",
       "      <td>0.000224</td>\n",
       "      <td>0.167474</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000571</td>\n",
       "      <td>-0.000352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_car_05_cat</th>\n",
       "      <td>7.422425e-05</td>\n",
       "      <td>0.000102</td>\n",
       "      <td>0.088719</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000283</td>\n",
       "      <td>-0.000135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_calc_14</th>\n",
       "      <td>5.537422e-05</td>\n",
       "      <td>0.000077</td>\n",
       "      <td>0.091520</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000214</td>\n",
       "      <td>-0.000103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_ind_04_cat</th>\n",
       "      <td>5.105397e-05</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.046691</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000158</td>\n",
       "      <td>-0.000056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_calc_20_bin</th>\n",
       "      <td>3.963517e-05</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>0.185247</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000221</td>\n",
       "      <td>-0.000141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_calc_13</th>\n",
       "      <td>3.343366e-05</td>\n",
       "      <td>0.000151</td>\n",
       "      <td>0.323645</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000345</td>\n",
       "      <td>-0.000278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_calc_08</th>\n",
       "      <td>2.927711e-05</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.126446</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000130</td>\n",
       "      <td>-0.000072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_car_10_cat</th>\n",
       "      <td>2.764009e-05</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.119350</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000120</td>\n",
       "      <td>-0.000064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_ind_08_bin</th>\n",
       "      <td>1.743206e-05</td>\n",
       "      <td>0.000576</td>\n",
       "      <td>0.474634</td>\n",
       "      <td>5</td>\n",
       "      <td>0.001203</td>\n",
       "      <td>-0.001168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_calc_10</th>\n",
       "      <td>1.430570e-05</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.337375</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000160</td>\n",
       "      <td>-0.000131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_ind_14</th>\n",
       "      <td>5.340941e-06</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.187454</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>-0.000019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_calc_16_bin</th>\n",
       "      <td>5.060452e-06</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.224433</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>-0.000023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_calc_07</th>\n",
       "      <td>4.592123e-06</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.279124</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>-0.000029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_calc_11</th>\n",
       "      <td>4.316885e-06</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.374307</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>-0.000054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_ind_12_bin</th>\n",
       "      <td>4.198559e-06</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.422804</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000097</td>\n",
       "      <td>-0.000089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_calc_02</th>\n",
       "      <td>4.025573e-06</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.095812</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>-0.000008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_calc_09</th>\n",
       "      <td>3.603863e-06</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.450021</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000128</td>\n",
       "      <td>-0.000120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_calc_19_bin</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_car_08_cat</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_ind_18_bin</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_ind_10_bin</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_ind_11_bin</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_ind_13_bin</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_calc_17_bin</th>\n",
       "      <td>-4.068055e-07</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.671247</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>-0.000004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_car_11</th>\n",
       "      <td>-1.339870e-06</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.517231</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000133</td>\n",
       "      <td>-0.000136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_calc_06</th>\n",
       "      <td>-2.035745e-06</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.874157</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>-0.000009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_calc_15_bin</th>\n",
       "      <td>-3.161655e-06</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.787425</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>-0.000020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_calc_04</th>\n",
       "      <td>-6.160314e-06</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.823701</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>-0.000033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_ind_01</th>\n",
       "      <td>-8.054058e-06</td>\n",
       "      <td>0.000886</td>\n",
       "      <td>0.507622</td>\n",
       "      <td>5</td>\n",
       "      <td>0.001816</td>\n",
       "      <td>-0.001832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_calc_01</th>\n",
       "      <td>-1.482007e-05</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.866970</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>-0.000068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_calc_12</th>\n",
       "      <td>-2.505452e-05</td>\n",
       "      <td>0.000145</td>\n",
       "      <td>0.640274</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000274</td>\n",
       "      <td>-0.000324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_car_06_cat</th>\n",
       "      <td>-3.188013e-05</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>0.780847</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000139</td>\n",
       "      <td>-0.000203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_calc_18_bin</th>\n",
       "      <td>-3.422127e-05</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.903844</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>-0.000135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_calc_05</th>\n",
       "      <td>-4.916165e-05</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.975107</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>-0.000131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_car_12</th>\n",
       "      <td>-6.252995e-05</td>\n",
       "      <td>0.000125</td>\n",
       "      <td>0.836444</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000196</td>\n",
       "      <td>-0.000321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_ind_09_bin</th>\n",
       "      <td>-9.688309e-05</td>\n",
       "      <td>0.000383</td>\n",
       "      <td>0.699134</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000691</td>\n",
       "      <td>-0.000885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_car_02_cat</th>\n",
       "      <td>-1.341999e-04</td>\n",
       "      <td>0.000249</td>\n",
       "      <td>0.852359</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000379</td>\n",
       "      <td>-0.000648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_car_04_cat</th>\n",
       "      <td>-1.516765e-04</td>\n",
       "      <td>0.000274</td>\n",
       "      <td>0.858630</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000411</td>\n",
       "      <td>-0.000715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_ind_07_bin</th>\n",
       "      <td>-1.807478e-04</td>\n",
       "      <td>0.002826</td>\n",
       "      <td>0.553401</td>\n",
       "      <td>5</td>\n",
       "      <td>0.005638</td>\n",
       "      <td>-0.006000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_car_09_cat</th>\n",
       "      <td>-1.047618e-03</td>\n",
       "      <td>0.000913</td>\n",
       "      <td>0.968858</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000833</td>\n",
       "      <td>-0.002928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_car_11_cat</th>\n",
       "      <td>-3.694727e-03</td>\n",
       "      <td>0.019432</td>\n",
       "      <td>0.653704</td>\n",
       "      <td>5</td>\n",
       "      <td>0.036315</td>\n",
       "      <td>-0.043705</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  importance    stddev   p_value  n  p99_high   p99_low\n",
       "ps_ind_05_cat   2.336078e-02  0.007582  0.001163  5  0.038971  0.007750\n",
       "ps_car_13       7.689422e-03  0.011673  0.107378  5  0.031725 -0.016346\n",
       "ps_reg_03       7.202950e-03  0.002470  0.001429  5  0.012289  0.002117\n",
       "ps_car_07_cat   4.274511e-03  0.007997  0.149022  5  0.020741 -0.012192\n",
       "ps_ind_03       3.842139e-03  0.002970  0.022226  5  0.009958 -0.002274\n",
       "ps_ind_17_bin   3.671786e-03  0.004612  0.074835  5  0.013169 -0.005825\n",
       "ps_reg_01       3.067102e-03  0.002393  0.022816  5  0.007993 -0.001859\n",
       "ps_ind_15       2.409103e-03  0.001559  0.012969  5  0.005620 -0.000801\n",
       "ps_car_01_cat   2.277008e-03  0.004534  0.162147  5  0.011613 -0.007059\n",
       "ps_ind_06_bin   1.987626e-03  0.005054  0.214437  5  0.012395 -0.008419\n",
       "ps_car_03_cat   1.599415e-03  0.002446  0.108797  5  0.006637 -0.003438\n",
       "ps_ind_16_bin   1.404765e-03  0.001391  0.043453  5  0.004270 -0.001460\n",
       "ps_reg_02       7.112528e-04  0.001528  0.178328  5  0.003857 -0.002434\n",
       "ps_car_14       3.650170e-04  0.000281  0.022066  5  0.000945 -0.000215\n",
       "ps_ind_02_cat   2.597998e-04  0.000294  0.059689  5  0.000865 -0.000346\n",
       "ps_calc_03      1.792607e-04  0.000418  0.195858  5  0.001040 -0.000681\n",
       "ps_car_15       1.097464e-04  0.000224  0.167474  5  0.000571 -0.000352\n",
       "ps_car_05_cat   7.422425e-05  0.000102  0.088719  5  0.000283 -0.000135\n",
       "ps_calc_14      5.537422e-05  0.000077  0.091520  5  0.000214 -0.000103\n",
       "ps_ind_04_cat   5.105397e-05  0.000052  0.046691  5  0.000158 -0.000056\n",
       "ps_calc_20_bin  3.963517e-05  0.000088  0.185247  5  0.000221 -0.000141\n",
       "ps_calc_13      3.343366e-05  0.000151  0.323645  5  0.000345 -0.000278\n",
       "ps_calc_08      2.927711e-05  0.000049  0.126446  5  0.000130 -0.000072\n",
       "ps_car_10_cat   2.764009e-05  0.000045  0.119350  5  0.000120 -0.000064\n",
       "ps_ind_08_bin   1.743206e-05  0.000576  0.474634  5  0.001203 -0.001168\n",
       "ps_calc_10      1.430570e-05  0.000071  0.337375  5  0.000160 -0.000131\n",
       "ps_ind_14       5.340941e-06  0.000012  0.187454  5  0.000030 -0.000019\n",
       "ps_calc_16_bin  5.060452e-06  0.000013  0.224433  5  0.000033 -0.000023\n",
       "ps_calc_07      4.592123e-06  0.000016  0.279124  5  0.000038 -0.000029\n",
       "ps_calc_11      4.316885e-06  0.000028  0.374307  5  0.000062 -0.000054\n",
       "ps_ind_12_bin   4.198559e-06  0.000045  0.422804  5  0.000097 -0.000089\n",
       "ps_calc_02      4.025573e-06  0.000006  0.095812  5  0.000016 -0.000008\n",
       "ps_calc_09      3.603863e-06  0.000060  0.450021  5  0.000128 -0.000120\n",
       "ps_calc_19_bin  0.000000e+00  0.000000  0.500000  5  0.000000  0.000000\n",
       "ps_car_08_cat   0.000000e+00  0.000000  0.500000  5  0.000000  0.000000\n",
       "ps_ind_18_bin   0.000000e+00  0.000000  0.500000  5  0.000000  0.000000\n",
       "ps_ind_10_bin   0.000000e+00  0.000000  0.500000  5  0.000000  0.000000\n",
       "ps_ind_11_bin   0.000000e+00  0.000000  0.500000  5  0.000000  0.000000\n",
       "ps_ind_13_bin   0.000000e+00  0.000000  0.500000  5  0.000000  0.000000\n",
       "ps_calc_17_bin -4.068055e-07  0.000002  0.671247  5  0.000004 -0.000004\n",
       "ps_car_11      -1.339870e-06  0.000065  0.517231  5  0.000133 -0.000136\n",
       "ps_calc_06     -2.035745e-06  0.000003  0.874157  5  0.000005 -0.000009\n",
       "ps_calc_15_bin -3.161655e-06  0.000008  0.787425  5  0.000013 -0.000020\n",
       "ps_calc_04     -6.160314e-06  0.000013  0.823701  5  0.000021 -0.000033\n",
       "ps_ind_01      -8.054058e-06  0.000886  0.507622  5  0.001816 -0.001832\n",
       "ps_calc_01     -1.482007e-05  0.000026  0.866970  5  0.000038 -0.000068\n",
       "ps_calc_12     -2.505452e-05  0.000145  0.640274  5  0.000274 -0.000324\n",
       "ps_car_06_cat  -3.188013e-05  0.000083  0.780847  5  0.000139 -0.000203\n",
       "ps_calc_18_bin -3.422127e-05  0.000049  0.903844  5  0.000066 -0.000135\n",
       "ps_calc_05     -4.916165e-05  0.000040  0.975107  5  0.000032 -0.000131\n",
       "ps_car_12      -6.252995e-05  0.000125  0.836444  5  0.000196 -0.000321\n",
       "ps_ind_09_bin  -9.688309e-05  0.000383  0.699134  5  0.000691 -0.000885\n",
       "ps_car_02_cat  -1.341999e-04  0.000249  0.852359  5  0.000379 -0.000648\n",
       "ps_car_04_cat  -1.516765e-04  0.000274  0.858630  5  0.000411 -0.000715\n",
       "ps_ind_07_bin  -1.807478e-04  0.002826  0.553401  5  0.005638 -0.006000\n",
       "ps_car_09_cat  -1.047618e-03  0.000913  0.968858  5  0.000833 -0.002928\n",
       "ps_car_11_cat  -3.694727e-03  0.019432  0.653704  5  0.036315 -0.043705"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.feature_importance(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44167d98",
   "metadata": {},
   "source": [
    "Computed via permutation-shuffling, these feature importance scores quantify the drop in predictive performance (of the already trained predictor) when one column’s values are randomly shuffled across rows. \n",
    "\n",
    "https://explained.ai/rf-importance/\n",
    "\n",
    "The top features in this list contribute most to AutoGluon’s accuracy (for predicting when/if a patient will be readmitted to the hospital). Features with non-positive importance score hardly contribute to the predictor’s accuracy, or may even be actively harmful to include in the data (consider removing these features from your data and calling `fit` again). These scores facilitate interpretability of the predictor’s global behavior (which features it relies on for all predictions) rather than local explanations that only rationalize one particular prediction.\n",
    "\n",
    "https://christophm.github.io/interpretable-ml-book/taxonomy-of-interpretability-methods.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0899f6b8",
   "metadata": {},
   "source": [
    "## Accelerating inference\n",
    "\n",
    "We describe multiple ways to reduce the time it takes for AutoGluon to produce predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c71139f",
   "metadata": {},
   "source": [
    "### Keeping models in memory\n",
    "\n",
    "By default, AutoGluon loads models into memory one at a time and only when they are needed for prediction. This strategy is robust for large stacked/bagged ensembles, but leads to slower prediction times. If you plan to repeatedly make predictions (e.g. on new datapoints one at a time rather than one large test dataset), you can first specify that all models required for inference should be loaded into memory as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e3fadb77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following 2 models were already persisted and will be ignored in the model loading process: ['LightGBM_BAG_L1', 'WeightedEnsemble_L2']\n",
      "No valid unpersisted models were specified to be persisted, so no change in model persistence was performed.\n",
      "/tmp/ipykernel_35752/3590748705.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  preds = preds.append(pred_numpy)\n",
      "/tmp/ipykernel_35752/3590748705.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  preds = preds.append(pred_numpy)\n",
      "/tmp/ipykernel_35752/3590748705.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  preds = preds.append(pred_numpy)\n",
      "/tmp/ipykernel_35752/3590748705.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  preds = preds.append(pred_numpy)\n",
      "/tmp/ipykernel_35752/3590748705.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  preds = preds.append(pred_numpy)\n",
      "/tmp/ipykernel_35752/3590748705.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  preds = preds.append(pred_numpy)\n",
      "/tmp/ipykernel_35752/3590748705.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  preds = preds.append(pred_numpy)\n",
      "/tmp/ipykernel_35752/3590748705.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  preds = preds.append(pred_numpy)\n",
      "/tmp/ipykernel_35752/3590748705.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  preds = preds.append(pred_numpy)\n",
      "/tmp/ipykernel_35752/3590748705.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  preds = preds.append(pred_numpy)\n",
      "/tmp/ipykernel_35752/3590748705.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  preds = preds.append(pred_numpy)\n",
      "/tmp/ipykernel_35752/3590748705.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  preds = preds.append(pred_numpy)\n",
      "/tmp/ipykernel_35752/3590748705.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  preds = preds.append(pred_numpy)\n",
      "/tmp/ipykernel_35752/3590748705.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  preds = preds.append(pred_numpy)\n",
      "/tmp/ipykernel_35752/3590748705.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  preds = preds.append(pred_numpy)\n",
      "/tmp/ipykernel_35752/3590748705.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  preds = preds.append(pred_numpy)\n",
      "/tmp/ipykernel_35752/3590748705.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  preds = preds.append(pred_numpy)\n",
      "/tmp/ipykernel_35752/3590748705.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  preds = preds.append(pred_numpy)\n",
      "/tmp/ipykernel_35752/3590748705.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  preds = preds.append(pred_numpy)\n",
      "/tmp/ipykernel_35752/3590748705.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  preds = preds.append(pred_numpy)\n",
      "/home/stever7/.local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Evaluation: roc_auc on test data: 0.6111111111111112\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"roc_auc\": 0.6111111111111112,\n",
      "    \"accuracy\": 0.9,\n",
      "    \"balanced_accuracy\": 0.5000000000000002,\n",
      "    \"mcc\": 0.0,\n",
      "    \"f1\": 0.0,\n",
      "    \"precision\": 0.0,\n",
      "    \"recall\": 0.0\n",
      "}\n",
      "Unpersisted 2 models: ['LightGBM_BAG_L1', 'WeightedEnsemble_L2']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions:             0         1\n",
      "0   0.956901  0.043099\n",
      "1   0.963648  0.036352\n",
      "2   0.968904  0.031096\n",
      "3   0.975422  0.024578\n",
      "4   0.971613  0.028387\n",
      "5   0.971882  0.028118\n",
      "6   0.971016  0.028984\n",
      "7   0.962172  0.037828\n",
      "8   0.969281  0.030719\n",
      "9   0.969197  0.030803\n",
      "10  0.971344  0.028656\n",
      "11  0.970507  0.029493\n",
      "12  0.966494  0.033506\n",
      "13  0.939654  0.060346\n",
      "14  0.959474  0.040526\n",
      "15  0.973707  0.026293\n",
      "16  0.965323  0.034677\n",
      "17  0.959917  0.040083\n",
      "18  0.974798  0.025202\n",
      "19  0.959847  0.040153\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['LightGBM_BAG_L1', 'WeightedEnsemble_L2']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.persist_models()\n",
    "\n",
    "num_test = 20\n",
    "# preds = np.array([''] * num_test, dtype='object')\n",
    "preds = pd.DataFrame()\n",
    "for i in range(num_test):\n",
    "    datapoint = test_data_nolabel.iloc[[i]]\n",
    "    # pred_numpy = predictor.predict(datapoint, as_pandas=False)\n",
    "    pred_numpy = predictor.predict_proba(datapoint, as_pandas=True)\n",
    "    # preds[i] = pred_numpy[0]\n",
    "    preds = preds.append(pred_numpy)  # use \"pd.concat\"\n",
    "\n",
    "# perf = predictor.evaluate_predictions(y_test[:num_test], preds, auxiliary_metrics=True)\n",
    "perf = predictor.evaluate_predictions(y_true=y_test[:num_test], y_pred=preds, auxiliary_metrics=True)\n",
    "print(\"Predictions: \", preds)\n",
    "\n",
    "predictor.unpersist_models()  # free memory by clearing models, future predict() calls will load models from disk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8947e9b",
   "metadata": {},
   "source": [
    "You can alternatively specify a particular model to persist via the models argument of `persist_models()`, or simply set `models='all'` to simultaneously load every single model that was trained during `fit`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec5b3f9",
   "metadata": {},
   "source": [
    "## Using smaller ensemble or faster model for prediction\n",
    "\n",
    "Without having to retrain any models, one can construct alternative ensembles that aggregate individual models’ predictions with different weighting schemes. These ensembles become smaller (and hence faster for prediction) if they assign nonzero weight to less models. You can produce a wide variety of ensembles with different accuracy-speed tradeoffs like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0f3cf05f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting model: WeightedEnsemble_L2Best ...\n",
      "\t0.624\t = Validation score   (roc_auc)\n",
      "\t0.1s\t = Training   runtime\n",
      "\t0.09s\t = Validation runtime\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alternative ensembles you can use for prediction: ['WeightedEnsemble_L2Best']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>score_val</th>\n",
       "      <th>pred_time_val</th>\n",
       "      <th>fit_time</th>\n",
       "      <th>pred_time_val_marginal</th>\n",
       "      <th>fit_time_marginal</th>\n",
       "      <th>stack_level</th>\n",
       "      <th>can_infer</th>\n",
       "      <th>fit_order</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LightGBM_BAG_L1</td>\n",
       "      <td>0.623974</td>\n",
       "      <td>0.744501</td>\n",
       "      <td>4.017492</td>\n",
       "      <td>0.744501</td>\n",
       "      <td>4.017492</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             model  score_val  pred_time_val  fit_time  \\\n",
       "0  LightGBM_BAG_L1   0.623974       0.744501  4.017492   \n",
       "\n",
       "   pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  \\\n",
       "0                0.744501           4.017492            1       True   \n",
       "\n",
       "   fit_order  \n",
       "0          1  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "additional_ensembles = predictor.fit_weighted_ensemble(expand_pareto_frontier=True)\n",
    "print(\"Alternative ensembles you can use for prediction:\", additional_ensembles)\n",
    "\n",
    "predictor.leaderboard(only_pareto_frontier=True, silent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b103a21d",
   "metadata": {},
   "source": [
    "The resulting leaderboard will contain the most accurate model for a given inference-latency. You can select whichever model exhibits acceptable latency from the leaderboard and use it for prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0b2dc660",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deleting model WeightedEnsemble_L2Best. All files under agModels-predict_porto_2/models/WeightedEnsemble_L2Best/ will be removed.\n"
     ]
    }
   ],
   "source": [
    "model_for_prediction = additional_ensembles[0]\n",
    "# predictions = predictor.predict(test_data, model=model_for_prediction)\n",
    "predictions = predictor.predict_proba(test_data, model=model_for_prediction)\n",
    "\n",
    "# delete these extra models so they don't affect rest of tutorial\n",
    "predictor.delete_models(models_to_delete=additional_ensembles, dry_run=False)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3cd551f",
   "metadata": {},
   "source": [
    "### Collapsing bagged ensembles via `refit_full`\n",
    "\n",
    "For an ensemble predictor trained with bagging (as done above), recall there ~10 bagged copies of each individual model trained on different train/validation folds. We can collapse this bag of ~10 models into a single model that’s fit to the full dataset, which can greatly reduce its memory/latency requirements (but may also reduce accuracy). Below we refit such a model for each original model but you can alternatively do this for just a particular model by specifying the model argument of `refit_full()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "13f54ac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Refitting models via `predictor.refit_full` using all of the data (combined train and validation)...\n",
      "\tModels trained in this way will have the suffix \"_FULL\" and have NaN validation score.\n",
      "\tThis process is not bound by time_limit, but should take less time than the original `predictor.fit` call.\n",
      "\tTo learn more, refer to the `.refit_full` method docstring which explains how \"_FULL\" models differ from normal models.\n",
      "Fitting 1 L1 models ...\n",
      "Fitting model: LightGBM_BAG_L1_FULL ...\n",
      "\t1.49s\t = Training   runtime\n",
      "Fitting model: WeightedEnsemble_L2_FULL | Skipping fit via cloning parent ...\n",
      "\t0.09s\t = Training   runtime\n",
      "Updated best model to \"WeightedEnsemble_L2_FULL\" (Previously \"WeightedEnsemble_L2\"). AutoGluon will default to using \"WeightedEnsemble_L2_FULL\" for predict() and predict_proba().\n",
      "Refit complete, total runtime = 2.57s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name of each refit-full model corresponding to a previous bagged ensemble:\n",
      "{'LightGBM_BAG_L1': 'LightGBM_BAG_L1_FULL', 'WeightedEnsemble_L2': 'WeightedEnsemble_L2_FULL'}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>score_test</th>\n",
       "      <th>score_val</th>\n",
       "      <th>pred_time_test</th>\n",
       "      <th>pred_time_val</th>\n",
       "      <th>fit_time</th>\n",
       "      <th>pred_time_test_marginal</th>\n",
       "      <th>pred_time_val_marginal</th>\n",
       "      <th>fit_time_marginal</th>\n",
       "      <th>stack_level</th>\n",
       "      <th>can_infer</th>\n",
       "      <th>fit_order</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LightGBM_BAG_L1</td>\n",
       "      <td>0.618328</td>\n",
       "      <td>0.623974</td>\n",
       "      <td>0.824968</td>\n",
       "      <td>0.744501</td>\n",
       "      <td>4.017492</td>\n",
       "      <td>0.824968</td>\n",
       "      <td>0.744501</td>\n",
       "      <td>4.017492</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>WeightedEnsemble_L2</td>\n",
       "      <td>0.618328</td>\n",
       "      <td>0.623974</td>\n",
       "      <td>0.830042</td>\n",
       "      <td>0.834036</td>\n",
       "      <td>4.108663</td>\n",
       "      <td>0.005074</td>\n",
       "      <td>0.089535</td>\n",
       "      <td>0.091171</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LightGBM_BAG_L1_FULL</td>\n",
       "      <td>0.616898</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.106917</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.491984</td>\n",
       "      <td>0.106917</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.491984</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>WeightedEnsemble_L2_FULL</td>\n",
       "      <td>0.616898</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.112218</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.583154</td>\n",
       "      <td>0.005301</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.091171</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      model  score_test  score_val  pred_time_test  \\\n",
       "0           LightGBM_BAG_L1    0.618328   0.623974        0.824968   \n",
       "1       WeightedEnsemble_L2    0.618328   0.623974        0.830042   \n",
       "2      LightGBM_BAG_L1_FULL    0.616898        NaN        0.106917   \n",
       "3  WeightedEnsemble_L2_FULL    0.616898        NaN        0.112218   \n",
       "\n",
       "   pred_time_val  fit_time  pred_time_test_marginal  pred_time_val_marginal  \\\n",
       "0       0.744501  4.017492                 0.824968                0.744501   \n",
       "1       0.834036  4.108663                 0.005074                0.089535   \n",
       "2            NaN  1.491984                 0.106917                     NaN   \n",
       "3            NaN  1.583154                 0.005301                     NaN   \n",
       "\n",
       "   fit_time_marginal  stack_level  can_infer  fit_order  \n",
       "0           4.017492            1       True          1  \n",
       "1           0.091171            2       True          2  \n",
       "2           1.491984            1       True          3  \n",
       "3           0.091171            2       True          4  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "refit_model_map = predictor.refit_full()\n",
    "print(\"Name of each refit-full model corresponding to a previous bagged ensemble:\")\n",
    "print(refit_model_map)\n",
    "predictor.leaderboard(test_data, silent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189ac881",
   "metadata": {},
   "source": [
    "This adds the refit-full models to the leaderboard and we can opt to use any of them for prediction just like any other model. Note `pred_time_test` and `pred_time_val` list the time taken to produce predictions with each model (in seconds) on the test/validation data. Since the refit-full models were trained using all of the data, there is no internal validation score (`score_val`) available for them. You can also call `refit_full()` with non-bagged models to refit the same models to your full dataset (there won’t be memory/latency gains in this case but test accuracy may improve)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5b72af",
   "metadata": {},
   "source": [
    "### Model distillation\n",
    "\n",
    "While computationally-favorable, single individual models will usually have lower accuracy than weighted/stacked/bagged ensembles. Model Distillation offers one way to retain the computational benefits of a single model, while enjoying some of the accuracy-boost that comes with ensembling. \n",
    "\n",
    "\"Fast, Accurate, and Simple Models for Tabular Data via Augmented Distillation\": https://arxiv.org/abs/2006.14284\n",
    "\n",
    "The idea is to train the individual model (which we can call the student) to mimic the predictions of the full stack ensemble (the teacher). Like `refit_full()`, the `distill()` function will produce additional models we can opt to use for prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "51c8b355",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Distilling with teacher='WeightedEnsemble_L2_FULL', teacher_preds=soft, augment_method=spunge ...\n",
      "SPUNGE: Augmenting training data with 100000 synthetic samples for distillation...\n",
      "Distilling with each of these student models: ['LightGBM_DSTL', 'NeuralNetMXNet_DSTL', 'CatBoost_DSTL', 'RandomForestMSE_DSTL', 'NeuralNetTorch_DSTL']\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBM_DSTL ... Training model for up to 30.0s of the 30.0s of remaining time.\n",
      "\tNote: model has different eval_metric than default.\n",
      "\t-0.0337\t = Validation score   (-mean_squared_error)\n",
      "\t2.62s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: NeuralNetMXNet_DSTL ... Training model for up to 27.27s of the 27.26s of remaining time.\n",
      "\tWarning: Exception caused NeuralNetMXNet_DSTL to fail during training (ImportError)... Skipping this model.\n",
      "\t\tUnable to import dependency mxnet. A quick tip is to install via `pip install mxnet --upgrade`, or `pip install mxnet_cu101 --upgrade`\n",
      "Fitting model: CatBoost_DSTL ... Training model for up to 26.86s of the 26.86s of remaining time.\n",
      "\tNote: model has different eval_metric than default.\n",
      "\t-0.0337\t = Validation score   (-mean_squared_error)\n",
      "\t18.77s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE_DSTL ... Training model for up to 7.97s of the 7.97s of remaining time.\n",
      "\tNote: model has different eval_metric than default.\n",
      "\t-0.0345\t = Validation score   (-mean_squared_error)\n",
      "\t116.34s\t = Training   runtime\n",
      "\t0.12s\t = Validation runtime\n",
      "Completed 1/20 k-fold bagging repeats ...\n",
      "Distilling with each of these student models: ['WeightedEnsemble_L2_DSTL']\n",
      "Fitting model: WeightedEnsemble_L2_DSTL ... Training model for up to 30.0s of the -110.27s of remaining time.\n",
      "\tNote: model has different eval_metric than default.\n",
      "\t-0.0337\t = Validation score   (-mean_squared_error)\n",
      "\t0.16s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Distilled model leaderboard:\n",
      "                      model  score_val  pred_time_val    fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0  WeightedEnsemble_L2_DSTL   0.599191       0.172532  137.877636                0.000668           0.155618            2       True          8\n",
      "1             CatBoost_DSTL   0.595678       0.027418   18.768441                0.027418          18.768441            1       True          6\n",
      "2             LightGBM_DSTL   0.585840       0.021080    2.617640                0.021080           2.617640            1       True          5\n",
      "3      RandomForestMSE_DSTL   0.567056       0.123366  116.335937                0.123366         116.335937            1       True          7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['LightGBM_DSTL', 'CatBoost_DSTL', 'RandomForestMSE_DSTL', 'WeightedEnsemble_L2_DSTL']\n",
      "predictions from LightGBM_DSTL: [0, 1]\n",
      "                      model  score_test  score_val  pred_time_test  pred_time_val    fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0             CatBoost_DSTL    0.630523   0.595678        0.170604       0.027418   18.768441                 0.170604                0.027418          18.768441            1       True          6\n",
      "1  WeightedEnsemble_L2_DSTL    0.626658   0.599191        1.228356       0.172532  137.877636                 0.021181                0.000668           0.155618            2       True          8\n",
      "2           LightGBM_BAG_L1    0.618328   0.623974        0.809814       0.744501    4.017492                 0.809814                0.744501           4.017492            1       True          1\n",
      "3       WeightedEnsemble_L2    0.618328   0.623974        0.816011       0.834036    4.108663                 0.006197                0.089535           0.091171            2       True          2\n",
      "4             LightGBM_DSTL    0.617487   0.585840        0.088338       0.021080    2.617640                 0.088338                0.021080           2.617640            1       True          5\n",
      "5      LightGBM_BAG_L1_FULL    0.616898        NaN        0.095858            NaN    1.491984                 0.095858                     NaN           1.491984            1       True          3\n",
      "6  WeightedEnsemble_L2_FULL    0.616898        NaN        0.100391            NaN    1.583154                 0.004533                     NaN           0.091171            2       True          4\n",
      "7      RandomForestMSE_DSTL    0.592676   0.567056        0.948233       0.123366  116.335937                 0.948233                0.123366         116.335937            1       True          7\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>score_test</th>\n",
       "      <th>score_val</th>\n",
       "      <th>pred_time_test</th>\n",
       "      <th>pred_time_val</th>\n",
       "      <th>fit_time</th>\n",
       "      <th>pred_time_test_marginal</th>\n",
       "      <th>pred_time_val_marginal</th>\n",
       "      <th>fit_time_marginal</th>\n",
       "      <th>stack_level</th>\n",
       "      <th>can_infer</th>\n",
       "      <th>fit_order</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CatBoost_DSTL</td>\n",
       "      <td>0.630523</td>\n",
       "      <td>0.595678</td>\n",
       "      <td>0.170604</td>\n",
       "      <td>0.027418</td>\n",
       "      <td>18.768441</td>\n",
       "      <td>0.170604</td>\n",
       "      <td>0.027418</td>\n",
       "      <td>18.768441</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>WeightedEnsemble_L2_DSTL</td>\n",
       "      <td>0.626658</td>\n",
       "      <td>0.599191</td>\n",
       "      <td>1.228356</td>\n",
       "      <td>0.172532</td>\n",
       "      <td>137.877636</td>\n",
       "      <td>0.021181</td>\n",
       "      <td>0.000668</td>\n",
       "      <td>0.155618</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LightGBM_BAG_L1</td>\n",
       "      <td>0.618328</td>\n",
       "      <td>0.623974</td>\n",
       "      <td>0.809814</td>\n",
       "      <td>0.744501</td>\n",
       "      <td>4.017492</td>\n",
       "      <td>0.809814</td>\n",
       "      <td>0.744501</td>\n",
       "      <td>4.017492</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>WeightedEnsemble_L2</td>\n",
       "      <td>0.618328</td>\n",
       "      <td>0.623974</td>\n",
       "      <td>0.816011</td>\n",
       "      <td>0.834036</td>\n",
       "      <td>4.108663</td>\n",
       "      <td>0.006197</td>\n",
       "      <td>0.089535</td>\n",
       "      <td>0.091171</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LightGBM_DSTL</td>\n",
       "      <td>0.617487</td>\n",
       "      <td>0.585840</td>\n",
       "      <td>0.088338</td>\n",
       "      <td>0.021080</td>\n",
       "      <td>2.617640</td>\n",
       "      <td>0.088338</td>\n",
       "      <td>0.021080</td>\n",
       "      <td>2.617640</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LightGBM_BAG_L1_FULL</td>\n",
       "      <td>0.616898</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.095858</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.491984</td>\n",
       "      <td>0.095858</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.491984</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>WeightedEnsemble_L2_FULL</td>\n",
       "      <td>0.616898</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.100391</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.583154</td>\n",
       "      <td>0.004533</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.091171</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>RandomForestMSE_DSTL</td>\n",
       "      <td>0.592676</td>\n",
       "      <td>0.567056</td>\n",
       "      <td>0.948233</td>\n",
       "      <td>0.123366</td>\n",
       "      <td>116.335937</td>\n",
       "      <td>0.948233</td>\n",
       "      <td>0.123366</td>\n",
       "      <td>116.335937</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      model  score_test  score_val  pred_time_test  \\\n",
       "0             CatBoost_DSTL    0.630523   0.595678        0.170604   \n",
       "1  WeightedEnsemble_L2_DSTL    0.626658   0.599191        1.228356   \n",
       "2           LightGBM_BAG_L1    0.618328   0.623974        0.809814   \n",
       "3       WeightedEnsemble_L2    0.618328   0.623974        0.816011   \n",
       "4             LightGBM_DSTL    0.617487   0.585840        0.088338   \n",
       "5      LightGBM_BAG_L1_FULL    0.616898        NaN        0.095858   \n",
       "6  WeightedEnsemble_L2_FULL    0.616898        NaN        0.100391   \n",
       "7      RandomForestMSE_DSTL    0.592676   0.567056        0.948233   \n",
       "\n",
       "   pred_time_val    fit_time  pred_time_test_marginal  pred_time_val_marginal  \\\n",
       "0       0.027418   18.768441                 0.170604                0.027418   \n",
       "1       0.172532  137.877636                 0.021181                0.000668   \n",
       "2       0.744501    4.017492                 0.809814                0.744501   \n",
       "3       0.834036    4.108663                 0.006197                0.089535   \n",
       "4       0.021080    2.617640                 0.088338                0.021080   \n",
       "5            NaN    1.491984                 0.095858                     NaN   \n",
       "6            NaN    1.583154                 0.004533                     NaN   \n",
       "7       0.123366  116.335937                 0.948233                0.123366   \n",
       "\n",
       "   fit_time_marginal  stack_level  can_infer  fit_order  \n",
       "0          18.768441            1       True          6  \n",
       "1           0.155618            2       True          8  \n",
       "2           4.017492            1       True          1  \n",
       "3           0.091171            2       True          2  \n",
       "4           2.617640            1       True          5  \n",
       "5           1.491984            1       True          3  \n",
       "6           0.091171            2       True          4  \n",
       "7         116.335937            1       True          7  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student_models = predictor.distill(time_limit=30)  # specify much longer time limit in real applications\n",
    "print(student_models)\n",
    "\n",
    "# preds_student = predictor.predict(test_data_nolabel, model=student_models[0])\n",
    "preds_student = predictor.predict_proba(test_data_nolabel, model=student_models[0])\n",
    "print(f\"predictions from {student_models[0]}:\", list(preds_student)[:5])\n",
    "predictor.leaderboard(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c123104d",
   "metadata": {},
   "source": [
    "### Faster presets or hyperparameters\n",
    "\n",
    "Instead of trying to speed up a cumbersome trained model at prediction time, if you know inference latency or memory will be an issue at the outset, then you can adjust the training process accordingly to ensure `fit()` does not produce unwieldy models.\n",
    "\n",
    "One option is to specify more lightweight presets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d86b2383",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20230702_174411/\"\n",
      "Preset alias specified: 'good_quality_faster_inference_only_refit' maps to 'good_quality'.\n",
      "Presets specified: ['good_quality_faster_inference_only_refit', 'optimize_for_deployment']\n",
      "Stack configuration (auto_stack=True): num_stack_levels=0, num_bag_folds=8, num_bag_sets=20\n",
      "Beginning AutoGluon training ... Time limit = 30s\n",
      "AutoGluon will save models to \"AutogluonModels/ag-20230702_174411/\"\n",
      "AutoGluon Version:  0.7.0\n",
      "Python Version:     3.9.11\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Thu May 4 15:21:22 UTC 2023\n",
      "Train Data Rows:    476170\n",
      "Train Data Columns: 57\n",
      "Label Column: target\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'binary' (because only two unique label-values observed).\n",
      "\t2 unique label values:  [0, 1]\n",
      "\tIf 'binary' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    152926.9 MB\n",
      "\tTrain Data (Original)  Memory Usage: 170.47 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 18 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('category', []) : 14 | ['ps_ind_02_cat', 'ps_ind_04_cat', 'ps_ind_05_cat', 'ps_car_01_cat', 'ps_car_02_cat', ...]\n",
      "\t\t('float', [])    : 11 | ['ps_reg_01', 'ps_reg_02', 'ps_reg_03', 'ps_car_11', 'ps_car_12', ...]\n",
      "\t\t('int', [])      : 32 | ['ps_ind_01', 'ps_ind_03', 'ps_ind_06_bin', 'ps_ind_07_bin', 'ps_ind_08_bin', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])  : 13 | ['ps_ind_02_cat', 'ps_ind_04_cat', 'ps_ind_05_cat', 'ps_car_01_cat', 'ps_car_02_cat', ...]\n",
      "\t\t('float', [])     : 11 | ['ps_reg_01', 'ps_reg_02', 'ps_reg_03', 'ps_car_11', 'ps_car_12', ...]\n",
      "\t\t('int', [])       : 15 | ['ps_ind_01', 'ps_ind_03', 'ps_ind_14', 'ps_ind_15', 'ps_calc_04', ...]\n",
      "\t\t('int', ['bool']) : 18 | ['ps_ind_06_bin', 'ps_ind_07_bin', 'ps_ind_08_bin', 'ps_ind_09_bin', 'ps_ind_10_bin', ...]\n",
      "\t2.4s = Fit runtime\n",
      "\t57 features in original data used to generate 57 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 113.81 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 2.97s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'roc_auc'\n",
      "\tThis metric expects predicted probabilities rather than predicted class labels, so you'll need to use predict_proba() instead of predict()\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 27.03s of the 27.03s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t0.6327\t = Validation score   (roc_auc)\n",
      "\t9.52s\t = Training   runtime\n",
      "\t1.43s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 3.52s of the 3.52s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t0.6276\t = Validation score   (roc_auc)\n",
      "\t4.48s\t = Training   runtime\n",
      "\t0.91s\t = Validation runtime\n",
      "Completed 1/20 k-fold bagging repeats ...\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 27.03s of the -7.33s of remaining time.\n",
      "\t0.6328\t = Validation score   (roc_auc)\n",
      "\t19.51s\t = Training   runtime\n",
      "\t0.09s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 57.58s ... Best model: \"WeightedEnsemble_L2\"\n",
      "Automatically performing refit_full as a post-fit operation (due to `.fit(..., refit_full=True)`\n",
      "Refitting models via `predictor.refit_full` using all of the data (combined train and validation)...\n",
      "\tModels trained in this way will have the suffix \"_FULL\" and have NaN validation score.\n",
      "\tThis process is not bound by time_limit, but should take less time than the original `predictor.fit` call.\n",
      "\tTo learn more, refer to the `.refit_full` method docstring which explains how \"_FULL\" models differ from normal models.\n",
      "Fitting 1 L1 models ...\n",
      "Fitting model: LightGBMXT_BAG_L1_FULL ...\n",
      "\t2.15s\t = Training   runtime\n",
      "Fitting 1 L1 models ...\n",
      "Fitting model: LightGBM_BAG_L1_FULL ...\n",
      "\t1.75s\t = Training   runtime\n",
      "Fitting model: WeightedEnsemble_L2_FULL | Skipping fit via cloning parent ...\n",
      "\t19.51s\t = Training   runtime\n",
      "Refit complete, total runtime = 6.27s\n",
      "Deleting model LightGBMXT_BAG_L1. All files under AutogluonModels/ag-20230702_174411/models/LightGBMXT_BAG_L1/ will be removed.\n",
      "Deleting model LightGBM_BAG_L1. All files under AutogluonModels/ag-20230702_174411/models/LightGBM_BAG_L1/ will be removed.\n",
      "Deleting model WeightedEnsemble_L2. All files under AutogluonModels/ag-20230702_174411/models/WeightedEnsemble_L2/ will be removed.\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20230702_174411/\")\n"
     ]
    }
   ],
   "source": [
    "presets = ['good_quality_faster_inference_only_refit', 'optimize_for_deployment']\n",
    "predictor_light = TabularPredictor(label=label, eval_metric=metric).fit(train_data, presets=presets, time_limit=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5d5615",
   "metadata": {},
   "source": [
    "Another option is to specify more lightweight hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "fc1c1f38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20230702_175205/\"\n",
      "Beginning AutoGluon training ... Time limit = 3600s\n",
      "AutoGluon will save models to \"AutogluonModels/ag-20230702_175205/\"\n",
      "AutoGluon Version:  0.7.0\n",
      "Python Version:     3.9.11\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Thu May 4 15:21:22 UTC 2023\n",
      "Train Data Rows:    476170\n",
      "Train Data Columns: 57\n",
      "Label Column: target\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'binary' (because only two unique label-values observed).\n",
      "\t2 unique label values:  [0, 1]\n",
      "\tIf 'binary' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    152776.55 MB\n",
      "\tTrain Data (Original)  Memory Usage: 170.47 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 18 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('category', []) : 14 | ['ps_ind_02_cat', 'ps_ind_04_cat', 'ps_ind_05_cat', 'ps_car_01_cat', 'ps_car_02_cat', ...]\n",
      "\t\t('float', [])    : 11 | ['ps_reg_01', 'ps_reg_02', 'ps_reg_03', 'ps_car_11', 'ps_car_12', ...]\n",
      "\t\t('int', [])      : 32 | ['ps_ind_01', 'ps_ind_03', 'ps_ind_06_bin', 'ps_ind_07_bin', 'ps_ind_08_bin', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])  : 13 | ['ps_ind_02_cat', 'ps_ind_04_cat', 'ps_ind_05_cat', 'ps_car_01_cat', 'ps_car_02_cat', ...]\n",
      "\t\t('float', [])     : 11 | ['ps_reg_01', 'ps_reg_02', 'ps_reg_03', 'ps_car_11', 'ps_car_12', ...]\n",
      "\t\t('int', [])       : 15 | ['ps_ind_01', 'ps_ind_03', 'ps_ind_14', 'ps_ind_15', 'ps_calc_04', ...]\n",
      "\t\t('int', ['bool']) : 18 | ['ps_ind_06_bin', 'ps_ind_07_bin', 'ps_ind_08_bin', 'ps_ind_09_bin', 'ps_ind_10_bin', ...]\n",
      "\t2.6s = Fit runtime\n",
      "\t57 features in original data used to generate 57 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 113.81 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 3.21s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'roc_auc'\n",
      "\tThis metric expects predicted probabilities rather than predicted class labels, so you'll need to use predict_proba() instead of predict()\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.01, Train Rows: 471408, Val Rows: 4762\n",
      "Fitting 7 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 3596.79s of the 3596.79s of remaining time.\n",
      "\t0.5919\t = Validation score   (roc_auc)\n",
      "\t2.23s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 3594.46s of the 3594.46s of remaining time.\n",
      "\t0.5818\t = Validation score   (roc_auc)\n",
      "\t1.87s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 3592.5s of the 3592.5s of remaining time.\n",
      "\t0.602\t = Validation score   (roc_auc)\n",
      "\t46.27s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 3546.09s of the 3546.09s of remaining time.\n",
      "No improvement since epoch 6: early stopping\n",
      "\t0.5908\t = Validation score   (roc_auc)\n",
      "\t331.03s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 3214.65s of the 3214.65s of remaining time.\n",
      "\t0.6015\t = Validation score   (roc_auc)\n",
      "\t7.8s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 3206.68s of the 3206.68s of remaining time.\n",
      "\t0.5942\t = Validation score   (roc_auc)\n",
      "\t262.27s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 2944.24s of the 2944.24s of remaining time.\n",
      "\t0.5871\t = Validation score   (roc_auc)\n",
      "\t3.11s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.0s of the 2940.95s of remaining time.\n",
      "\t0.6057\t = Validation score   (roc_auc)\n",
      "\t0.78s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 660.93s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20230702_175205/\")\n"
     ]
    }
   ],
   "source": [
    "# time_limit=30 -> 3600\n",
    "\n",
    "predictor_light = TabularPredictor(\n",
    "    label=label, \n",
    "    eval_metric=metric\n",
    ").fit(\n",
    "    train_data, \n",
    "    hyperparameters='very_light', \n",
    "    time_limit=3600\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163ef19b",
   "metadata": {},
   "source": [
    "Here you can set hyperparameters to either 'light', 'very_light', or 'toy' to obtain progressively smaller (but less accurate) models and predictors. Advanced users may instead try manually specifying particular models’ hyperparameters in order to make them faster/smaller.\n",
    "\n",
    "Finally, you may also exclude specific unwieldy models from being trained at all. Below we exclude models that tend to be slower (K Nearest Neighbors, Neural Network, models with custom larger-than-default hyperparameters):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4d60d7cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20230702_180857/\"\n",
      "Beginning AutoGluon training ... Time limit = 3600s\n",
      "AutoGluon will save models to \"AutogluonModels/ag-20230702_180857/\"\n",
      "AutoGluon Version:  0.7.0\n",
      "Python Version:     3.9.11\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Thu May 4 15:21:22 UTC 2023\n",
      "Train Data Rows:    476170\n",
      "Train Data Columns: 57\n",
      "Label Column: target\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'binary' (because only two unique label-values observed).\n",
      "\t2 unique label values:  [0, 1]\n",
      "\tIf 'binary' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    152689.08 MB\n",
      "\tTrain Data (Original)  Memory Usage: 170.47 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 18 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('category', []) : 14 | ['ps_ind_02_cat', 'ps_ind_04_cat', 'ps_ind_05_cat', 'ps_car_01_cat', 'ps_car_02_cat', ...]\n",
      "\t\t('float', [])    : 11 | ['ps_reg_01', 'ps_reg_02', 'ps_reg_03', 'ps_car_11', 'ps_car_12', ...]\n",
      "\t\t('int', [])      : 32 | ['ps_ind_01', 'ps_ind_03', 'ps_ind_06_bin', 'ps_ind_07_bin', 'ps_ind_08_bin', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])  : 13 | ['ps_ind_02_cat', 'ps_ind_04_cat', 'ps_ind_05_cat', 'ps_car_01_cat', 'ps_car_02_cat', ...]\n",
      "\t\t('float', [])     : 11 | ['ps_reg_01', 'ps_reg_02', 'ps_reg_03', 'ps_car_11', 'ps_car_12', ...]\n",
      "\t\t('int', [])       : 15 | ['ps_ind_01', 'ps_ind_03', 'ps_ind_14', 'ps_ind_15', 'ps_calc_04', ...]\n",
      "\t\t('int', ['bool']) : 18 | ['ps_ind_06_bin', 'ps_ind_07_bin', 'ps_ind_08_bin', 'ps_ind_09_bin', 'ps_ind_10_bin', ...]\n",
      "\t2.6s = Fit runtime\n",
      "\t57 features in original data used to generate 57 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 113.81 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 3.12s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'roc_auc'\n",
      "\tThis metric expects predicted probabilities rather than predicted class labels, so you'll need to use predict_proba() instead of predict()\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.01, Train Rows: 471408, Val Rows: 4762\n",
      "Excluded Model Types: ['KNN', 'NN', 'custom']\n",
      "\tFound 'KNN' model in hyperparameters, but 'KNN' is present in `excluded_model_types` and will be removed.\n",
      "\tFound 'KNN' model in hyperparameters, but 'KNN' is present in `excluded_model_types` and will be removed.\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 3596.88s of the 3596.88s of remaining time.\n",
      "\t0.5919\t = Validation score   (roc_auc)\n",
      "\t2.06s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 3594.7s of the 3594.7s of remaining time.\n",
      "\t0.5818\t = Validation score   (roc_auc)\n",
      "\t1.87s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ... Training model for up to 3592.74s of the 3592.73s of remaining time.\n",
      "\t0.583\t = Validation score   (roc_auc)\n",
      "\t16.06s\t = Training   runtime\n",
      "\t0.17s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ... Training model for up to 3574.42s of the 3574.42s of remaining time.\n",
      "\t0.5819\t = Validation score   (roc_auc)\n",
      "\t15.69s\t = Training   runtime\n",
      "\t0.17s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 3556.55s of the 3556.55s of remaining time.\n",
      "\t0.602\t = Validation score   (roc_auc)\n",
      "\t47.27s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini ... Training model for up to 3509.13s of the 3509.13s of remaining time.\n",
      "\t0.5851\t = Validation score   (roc_auc)\n",
      "\t9.65s\t = Training   runtime\n",
      "\t0.16s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr ... Training model for up to 3497.25s of the 3497.25s of remaining time.\n",
      "\t0.6018\t = Validation score   (roc_auc)\n",
      "\t10.36s\t = Training   runtime\n",
      "\t0.17s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 3484.72s of the 3484.72s of remaining time.\n",
      "No improvement since epoch 6: early stopping\n",
      "\t0.5908\t = Validation score   (roc_auc)\n",
      "\t326.71s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 3157.56s of the 3157.56s of remaining time.\n",
      "\t0.6015\t = Validation score   (roc_auc)\n",
      "\t7.68s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 3149.69s of the 3149.69s of remaining time.\n",
      "\t0.5942\t = Validation score   (roc_auc)\n",
      "\t254.41s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 2895.13s of the 2895.13s of remaining time.\n",
      "\t0.5871\t = Validation score   (roc_auc)\n",
      "\t3.1s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.0s of the 2891.86s of remaining time.\n",
      "\t0.6123\t = Validation score   (roc_auc)\n",
      "\t1.17s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 710.46s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20230702_180857/\")\n"
     ]
    }
   ],
   "source": [
    "# time_limit=30 -> 3600 \n",
    "    \n",
    "excluded_model_types = ['KNN', 'NN', 'custom']\n",
    "\n",
    "predictor_light = TabularPredictor(label=label, eval_metric=metric).fit(\n",
    "    train_data, \n",
    "    excluded_model_types=excluded_model_types, \n",
    "    time_limit=3600\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63730a9e",
   "metadata": {},
   "source": [
    "## If you encounter memory issues\n",
    "\n",
    "To reduce memory usage during training, you may try each of the following strategies individually or combinations of them (these may harm accuracy):\n",
    "\n",
    "- In `fit()`, set `num_bag_sets=1` (can also try values greater than 1 to harm accuracy less)\n",
    "\n",
    "- In `fit()`, set `excluded_model_types=['KNN', 'XT', 'RF']` (or some subset of these models)\n",
    "\n",
    "- Try different `presets` in `fit()`\n",
    "\n",
    "- In `fit()`, set `hyperparameters='light'` or `hyperparameters='very_light'`\n",
    "\n",
    "- Text fields in your table require substantial memory for N-gram featurization. To mitigate this in `fit()`, you can either:\n",
    "    + (1) add `'ignore_text'` to your `presets` list (to ignore text features), or \n",
    "    + (2) specify the argument:\n",
    "\n",
    "```\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from autogluon.features.generators import AutoMLPipelineFeatureGenerator\n",
    "\n",
    "feature_generator = AutoMLPipelineFeatureGenerator(\n",
    "    vectorizer=CountVectorizer(\n",
    "        min_df=30, \n",
    "        ngram_range=(1, 3), \n",
    "        max_features=MAX_NGRAM, \n",
    "        dtype=np.uint8\n",
    "    )\n",
    ")\n",
    "```\n",
    "\n",
    "where `MAX_NGRAM=1000` say (try various values under 10000 to reduce the number of N-gram features used to represent each text field).\n",
    "\n",
    "In addition to reducing memory usage, many of the above strategies can also be used to reduce training times.\n",
    "\n",
    "To reduce memory usage during inference:\n",
    "\n",
    "- If trying to produce predictions for a large test dataset, break the test data into smaller chunks as demonstrated in FAQ.\n",
    "    + https://auto.gluon.ai/0.1.0/tutorials/tabular_prediction/tabular-faq.html#sec-faq\n",
    "\n",
    "- If models have been previously persisted in memory but inference-speed is not a major concern, call `predictor.unpersist_models()`\n",
    "\n",
    "- If models have been previously persisted in memory, bagging was used in `fit()`, and inference-speed is a concern: call `predictor.refit_full()` and use one of the refit-full models for prediction (ensure this is the only model persisted in memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e692db1",
   "metadata": {},
   "source": [
    "## If you encounter disk space issues\n",
    "\n",
    "To reduce disk usage, you may try each of the following strategies individually or combinations of them:\n",
    "\n",
    "- Make sure to delete all `predictor.path` folders from previous `fit()` runs\n",
    "    + These can eat up your free space if you call `fit()` many times\n",
    "    + If you didn’t specify path, AutoGluon still automatically saved its models to a folder called: `“AutogluonModels/ag-[TIMESTAMP]”`, where TIMESTAMP records when `fit()` was called, so make sure to also delete these folders if you run low on free space\n",
    "\n",
    "- Call `predictor.save_space()` to delete auxiliary files produced during `fit()`\n",
    "\n",
    "- Call `predictor.delete_models(models_to_keep='best', dry_run=False)` if you only intend to use this predictor for inference going forward (will delete files required for non-prediction-related functionality like `fit_summary`)\n",
    "\n",
    "- In `fit()`, you can add `'optimize_for_deployment'` to the presets list, which will automatically invoke the previous two strategies after training\n",
    "\n",
    "Most of the above strategies to reduce memory usage will also reduce disk usage (but may harm accuracy)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845980f0",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "The following paper describes how AutoGluon internally operates on tabular data:\n",
    "\n",
    "Erickson et al. \"AutoGluon-Tabular: Robust and Accurate AutoML for Structured Data\". Arxiv, 2020: https://arxiv.org/abs/2003.06505"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
