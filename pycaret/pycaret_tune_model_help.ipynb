{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d77ce98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00745c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycaret.classification import *\n",
    "\n",
    "import pycaret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5a55f46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function create_model in module pycaret.classification.functional:\n",
      "\n",
      "create_model(estimator: Union[str, Any], fold: Union[int, Any, NoneType] = None, round: int = 4, cross_validation: bool = True, fit_kwargs: Optional[dict] = None, groups: Union[str, Any, NoneType] = None, probability_threshold: Optional[float] = None, experiment_custom_tags: Optional[Dict[str, Any]] = None, engine: Optional[str] = None, verbose: bool = True, return_train_score: bool = False, **kwargs) -> Any\n",
      "    This function trains and evaluates the performance of a given estimator\n",
      "    using cross validation. The output of this function is a score grid with\n",
      "    CV scores by fold. Metrics evaluated during CV can be accessed using the\n",
      "    ``get_metrics`` function. Custom metrics can be added or removed using\n",
      "    ``add_metric`` and ``remove_metric`` function. All the available models\n",
      "    can be accessed using the ``models`` function.\n",
      "    \n",
      "    Example\n",
      "    -------\n",
      "    >>> from pycaret.datasets import get_data\n",
      "    >>> juice = get_data('juice')\n",
      "    >>> from pycaret.classification import *\n",
      "    >>> exp_name = setup(data = juice,  target = 'Purchase')\n",
      "    >>> lr = create_model('lr')\n",
      "    \n",
      "    \n",
      "    estimator: str or scikit-learn compatible object\n",
      "        ID of an estimator available in model library or pass an untrained\n",
      "        model object consistent with scikit-learn API. Estimators available\n",
      "        in the model library (ID - Name):\n",
      "    \n",
      "        * 'lr' - Logistic Regression\n",
      "        * 'knn' - K Neighbors Classifier\n",
      "        * 'nb' - Naive Bayes\n",
      "        * 'dt' - Decision Tree Classifier\n",
      "        * 'svm' - SVM - Linear Kernel\n",
      "        * 'rbfsvm' - SVM - Radial Kernel\n",
      "        * 'gpc' - Gaussian Process Classifier\n",
      "        * 'mlp' - MLP Classifier\n",
      "        * 'ridge' - Ridge Classifier\n",
      "        * 'rf' - Random Forest Classifier\n",
      "        * 'qda' - Quadratic Discriminant Analysis\n",
      "        * 'ada' - Ada Boost Classifier\n",
      "        * 'gbc' - Gradient Boosting Classifier\n",
      "        * 'lda' - Linear Discriminant Analysis\n",
      "        * 'et' - Extra Trees Classifier\n",
      "        * 'xgboost' - Extreme Gradient Boosting\n",
      "        * 'lightgbm' - Light Gradient Boosting Machine\n",
      "        * 'catboost' - CatBoost Classifier\n",
      "    \n",
      "    \n",
      "    fold: int or scikit-learn compatible CV generator, default = None\n",
      "        Controls cross-validation. If None, the CV generator in the ``fold_strategy``\n",
      "        parameter of the ``setup`` function is used. When an integer is passed,\n",
      "        it is interpreted as the 'n_splits' parameter of the CV generator in the\n",
      "        ``setup`` function.\n",
      "    \n",
      "    \n",
      "    round: int, default = 4\n",
      "        Number of decimal places the metrics in the score grid will be rounded to.\n",
      "    \n",
      "    \n",
      "    cross_validation: bool, default = True\n",
      "        When set to False, metrics are evaluated on holdout set. ``fold`` param\n",
      "        is ignored when cross_validation is set to False.\n",
      "    \n",
      "    \n",
      "    fit_kwargs: dict, default = {} (empty dict)\n",
      "        Dictionary of arguments passed to the fit method of the model.\n",
      "    \n",
      "    \n",
      "    groups: str or array-like, with shape (n_samples,), default = None\n",
      "        Optional group labels when GroupKFold is used for the cross validation.\n",
      "        It takes an array with shape (n_samples, ) where n_samples is the number\n",
      "        of rows in training dataset. When string is passed, it is interpreted as\n",
      "        the column name in the dataset containing group labels.\n",
      "    \n",
      "    \n",
      "    probability_threshold: float, default = None\n",
      "        Threshold for converting predicted probability to class label.\n",
      "        It defaults to 0.5 for all classifiers unless explicitly defined\n",
      "        in this parameter. Only applicable for binary classification.\n",
      "    \n",
      "    \n",
      "    experiment_custom_tags: dict, default = None\n",
      "        Dictionary of tag_name: String -> value: (String, but will be string-ified\n",
      "        if not) passed to the mlflow.set_tags to add new custom tags for the experiment.\n",
      "    \n",
      "    \n",
      "    verbose: bool, default = True\n",
      "        Score grid is not printed when verbose is set to False.\n",
      "    \n",
      "    \n",
      "    engine: Optional[str] = None\n",
      "        The execution engine to use for the model, e.g. for Logistic Regression (\"lr\"), users can\n",
      "        switch between \"sklearn\" and \"sklearnex\" by specifying\n",
      "        `engine=\"sklearnex\"`.\n",
      "    \n",
      "    \n",
      "    return_train_score: bool, default = False\n",
      "        If False, returns the CV Validation scores only.\n",
      "        If True, returns the CV training scores along with the CV validation scores.\n",
      "        This is useful when the user wants to do bias-variance tradeoff. A high CV\n",
      "        training score with a low corresponding CV validation score indicates overfitting.\n",
      "    \n",
      "    \n",
      "    **kwargs:\n",
      "        Additional keyword arguments to pass to the estimator.\n",
      "    \n",
      "    \n",
      "    Returns:\n",
      "        Trained Model\n",
      "    \n",
      "    \n",
      "    Warnings\n",
      "    --------\n",
      "    - AUC for estimators that does not support 'predict_proba' is shown as 0.0000.\n",
      "    \n",
      "    - Models are not logged on the ``MLFlow`` server when ``cross_validation`` param\n",
      "      is set to False.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(create_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9b2fde3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function tune_model in module pycaret.classification.functional:\n",
      "\n",
      "tune_model(estimator, fold: Union[int, Any, NoneType] = None, round: int = 4, n_iter: int = 10, custom_grid: Union[Dict[str, list], Any, NoneType] = None, optimize: str = 'Accuracy', custom_scorer=None, search_library: str = 'scikit-learn', search_algorithm: Optional[str] = None, early_stopping: Any = False, early_stopping_max_iters: int = 10, choose_better: bool = True, fit_kwargs: Optional[dict] = None, groups: Union[str, Any, NoneType] = None, return_tuner: bool = False, verbose: bool = True, tuner_verbose: Union[int, bool] = True, return_train_score: bool = False, **kwargs) -> Any\n",
      "    This function tunes the hyperparameters of a given estimator. The output of\n",
      "    this function is a score grid with CV scores by fold of the best selected\n",
      "    model based on ``optimize`` parameter. Metrics evaluated during CV can be\n",
      "    accessed using the ``get_metrics`` function. Custom metrics can be added\n",
      "    or removed using ``add_metric`` and ``remove_metric`` function.\n",
      "    \n",
      "    Example\n",
      "    -------\n",
      "    >>> from pycaret.datasets import get_data\n",
      "    >>> juice = get_data('juice')\n",
      "    >>> from pycaret.classification import *\n",
      "    >>> exp_name = setup(data = juice,  target = 'Purchase')\n",
      "    >>> lr = create_model('lr')\n",
      "    >>> tuned_lr = tune_model(lr)\n",
      "    \n",
      "    \n",
      "    estimator: scikit-learn compatible object\n",
      "        Trained model object\n",
      "    \n",
      "    \n",
      "    fold: int or scikit-learn compatible CV generator, default = None\n",
      "        Controls cross-validation. If None, the CV generator in the ``fold_strategy``\n",
      "        parameter of the ``setup`` function is used. When an integer is passed,\n",
      "        it is interpreted as the 'n_splits' parameter of the CV generator in the\n",
      "        ``setup`` function.\n",
      "    \n",
      "    \n",
      "    round: int, default = 4\n",
      "        Number of decimal places the metrics in the score grid will be rounded to.\n",
      "    \n",
      "    \n",
      "    n_iter: int, default = 10\n",
      "        Number of iterations in the grid search. Increasing 'n_iter' may improve\n",
      "        model performance but also increases the training time.\n",
      "    \n",
      "    \n",
      "    custom_grid: dictionary, default = None\n",
      "        To define custom search space for hyperparameters, pass a dictionary with\n",
      "        parameter name and values to be iterated. Custom grids must be in a format\n",
      "        supported by the defined ``search_library``.\n",
      "    \n",
      "    \n",
      "    optimize: str, default = 'Accuracy'\n",
      "        Metric name to be evaluated for hyperparameter tuning. It also accepts custom\n",
      "        metrics that are added through the ``add_metric`` function.\n",
      "    \n",
      "    \n",
      "    custom_scorer: object, default = None\n",
      "        custom scoring strategy can be passed to tune hyperparameters of the model.\n",
      "        It must be created using ``sklearn.make_scorer``. It is equivalent of adding\n",
      "        custom metric using the ``add_metric`` function and passing the name of the\n",
      "        custom metric in the ``optimize`` parameter.\n",
      "        Will be deprecated in future.\n",
      "    \n",
      "    \n",
      "    search_library: str, default = 'scikit-learn'\n",
      "        The search library used for tuning hyperparameters. Possible values:\n",
      "    \n",
      "        - 'scikit-learn' - default, requires no further installation\n",
      "            https://github.com/scikit-learn/scikit-learn\n",
      "    \n",
      "        - 'scikit-optimize' - ``pip install scikit-optimize``\n",
      "            https://scikit-optimize.github.io/stable/\n",
      "    \n",
      "        - 'tune-sklearn' - ``pip install tune-sklearn ray[tune]``\n",
      "            https://github.com/ray-project/tune-sklearn\n",
      "    \n",
      "        - 'optuna' - ``pip install optuna``\n",
      "            https://optuna.org/\n",
      "    \n",
      "    \n",
      "    search_algorithm: str, default = None\n",
      "        The search algorithm depends on the ``search_library`` parameter.\n",
      "        Some search algorithms require additional libraries to be installed.\n",
      "        If None, will use search library-specific default algorithm.\n",
      "    \n",
      "        - 'scikit-learn' possible values:\n",
      "            - 'random' : random grid search (default)\n",
      "            - 'grid' : grid search\n",
      "    \n",
      "        - 'scikit-optimize' possible values:\n",
      "            - 'bayesian' : Bayesian search (default)\n",
      "    \n",
      "        - 'tune-sklearn' possible values:\n",
      "            - 'random' : random grid search (default)\n",
      "            - 'grid' : grid search\n",
      "            - 'bayesian' : ``pip install scikit-optimize``\n",
      "            - 'hyperopt' : ``pip install hyperopt``\n",
      "            - 'optuna' : ``pip install optuna``\n",
      "            - 'bohb' : ``pip install hpbandster ConfigSpace``\n",
      "    \n",
      "        - 'optuna' possible values:\n",
      "            - 'random' : randomized search\n",
      "            - 'tpe' : Tree-structured Parzen Estimator search (default)\n",
      "    \n",
      "    \n",
      "    early_stopping: bool or str or object, default = False\n",
      "        Use early stopping to stop fitting to a hyperparameter configuration\n",
      "        if it performs poorly. Ignored when ``search_library`` is scikit-learn,\n",
      "        or if the estimator does not have 'partial_fit' attribute. If False or\n",
      "        None, early stopping will not be used. Can be either an object accepted\n",
      "        by the search library or one of the following:\n",
      "    \n",
      "        - 'asha' for Asynchronous Successive Halving Algorithm\n",
      "        - 'hyperband' for Hyperband\n",
      "        - 'median' for Median Stopping Rule\n",
      "        - If False or None, early stopping will not be used.\n",
      "    \n",
      "    \n",
      "    early_stopping_max_iters: int, default = 10\n",
      "        Maximum number of epochs to run for each sampled configuration.\n",
      "        Ignored if ``early_stopping`` is False or None.\n",
      "    \n",
      "    \n",
      "    choose_better: bool, default = True\n",
      "        When set to True, the returned object is always better performing. The\n",
      "        metric used for comparison is defined by the ``optimize`` parameter.\n",
      "    \n",
      "    \n",
      "    fit_kwargs: dict, default = {} (empty dict)\n",
      "        Dictionary of arguments passed to the fit method of the tuner.\n",
      "    \n",
      "    \n",
      "    groups: str or array-like, with shape (n_samples,), default = None\n",
      "        Optional group labels when GroupKFold is used for the cross validation.\n",
      "        It takes an array with shape (n_samples, ) where n_samples is the number\n",
      "        of rows in training dataset. When string is passed, it is interpreted as\n",
      "        the column name in the dataset containing group labels.\n",
      "    \n",
      "    \n",
      "    return_tuner: bool, default = False\n",
      "        When set to True, will return a tuple of (model, tuner_object).\n",
      "    \n",
      "    \n",
      "    verbose: bool, default = True\n",
      "        Score grid is not printed when verbose is set to False.\n",
      "    \n",
      "    \n",
      "    tuner_verbose: bool or in, default = True\n",
      "        If True or above 0, will print messages from the tuner. Higher values\n",
      "        print more messages. Ignored when ``verbose`` param is False.\n",
      "    \n",
      "    \n",
      "    return_train_score: bool, default = False\n",
      "        If False, returns the CV Validation scores only.\n",
      "        If True, returns the CV training scores along with the CV validation scores.\n",
      "        This is useful when the user wants to do bias-variance tradeoff. A high CV\n",
      "        training score with a low corresponding CV validation score indicates overfitting.\n",
      "    \n",
      "    \n",
      "    **kwargs:\n",
      "        Additional keyword arguments to pass to the optimizer.\n",
      "    \n",
      "    \n",
      "    Returns:\n",
      "        Trained Model and Optional Tuner Object when ``return_tuner`` is True.\n",
      "    \n",
      "    \n",
      "    Warnings\n",
      "    --------\n",
      "    - Using 'grid' as ``search_algorithm`` may result in very long computation.\n",
      "      Only recommended with smaller search spaces that can be defined in the\n",
      "      ``custom_grid`` parameter.\n",
      "    \n",
      "    - ``search_library`` 'tune-sklearn' does not support GPU models.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tune_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc37be3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function compare_models in module pycaret.classification.functional:\n",
      "\n",
      "compare_models(include: Optional[List[Union[str, Any]]] = None, exclude: Optional[List[str]] = None, fold: Union[int, Any, NoneType] = None, round: int = 4, cross_validation: bool = True, sort: str = 'Accuracy', n_select: int = 1, budget_time: Optional[float] = None, turbo: bool = True, errors: str = 'ignore', fit_kwargs: Optional[dict] = None, groups: Union[str, Any, NoneType] = None, experiment_custom_tags: Optional[Dict[str, Any]] = None, probability_threshold: Optional[float] = None, engine: Optional[Dict[str, str]] = None, verbose: bool = True, parallel: Optional[pycaret.internal.parallel.parallel_backend.ParallelBackend] = None) -> Union[Any, List[Any]]\n",
      "    This function trains and evaluates performance of all estimators available in the\n",
      "    model library using cross validation. The output of this function is a score grid\n",
      "    with average cross validated scores. Metrics evaluated during CV can be accessed\n",
      "    using the ``get_metrics`` function. Custom metrics can be added or removed using\n",
      "    ``add_metric`` and ``remove_metric`` function.\n",
      "    \n",
      "    Example\n",
      "    -------\n",
      "    >>> from pycaret.datasets import get_data\n",
      "    >>> juice = get_data('juice')\n",
      "    >>> from pycaret.classification import *\n",
      "    >>> exp_name = setup(data = juice,  target = 'Purchase')\n",
      "    >>> best_model = compare_models()\n",
      "    \n",
      "    \n",
      "    include: list of str or scikit-learn compatible object, default = None\n",
      "        To train and evaluate select models, list containing model ID or scikit-learn\n",
      "        compatible object can be passed in include param. To see a list of all models\n",
      "        available in the model library use the ``models`` function.\n",
      "    \n",
      "    \n",
      "    exclude: list of str, default = None\n",
      "        To omit certain models from training and evaluation, pass a list containing\n",
      "        model id in the exclude parameter. To see a list of all models available\n",
      "        in the model library use the ``models`` function.\n",
      "    \n",
      "    \n",
      "    fold: int or scikit-learn compatible CV generator, default = None\n",
      "        Controls cross-validation. If None, the CV generator in the ``fold_strategy``\n",
      "        parameter of the ``setup`` function is used. When an integer is passed,\n",
      "        it is interpreted as the 'n_splits' parameter of the CV generator in the\n",
      "        ``setup`` function.\n",
      "    \n",
      "    \n",
      "    round: int, default = 4\n",
      "        Number of decimal places the metrics in the score grid will be rounded to.\n",
      "    \n",
      "    \n",
      "    cross_validation: bool, default = True\n",
      "        When set to False, metrics are evaluated on holdout set. ``fold`` param\n",
      "        is ignored when cross_validation is set to False.\n",
      "    \n",
      "    \n",
      "    sort: str, default = 'Accuracy'\n",
      "        The sort order of the score grid. It also accepts custom metrics that are\n",
      "        added through the ``add_metric`` function.\n",
      "    \n",
      "    \n",
      "    n_select: int, default = 1\n",
      "        Number of top_n models to return. For example, to select top 3 models use\n",
      "        n_select = 3.\n",
      "    \n",
      "    \n",
      "    budget_time: int or float, default = None\n",
      "        If not None, will terminate execution of the function after budget_time\n",
      "        minutes have passed and return results up to that point.\n",
      "    \n",
      "    \n",
      "    turbo: bool, default = True\n",
      "        When set to True, it excludes estimators with longer training times. To\n",
      "        see which algorithms are excluded use the ``models`` function.\n",
      "    \n",
      "    \n",
      "    errors: str, default = 'ignore'\n",
      "        When set to 'ignore', will skip the model with exceptions and continue.\n",
      "        If 'raise', will break the function when exceptions are raised.\n",
      "    \n",
      "    \n",
      "    fit_kwargs: dict, default = {} (empty dict)\n",
      "        Dictionary of arguments passed to the fit method of the model.\n",
      "    \n",
      "    \n",
      "    groups: str or array-like, with shape (n_samples,), default = None\n",
      "        Optional group labels when 'GroupKFold' is used for the cross validation.\n",
      "        It takes an array with shape (n_samples, ) where n_samples is the number\n",
      "        of rows in the training dataset. When string is passed, it is interpreted\n",
      "        as the column name in the dataset containing group labels.\n",
      "    \n",
      "    \n",
      "    experiment_custom_tags: dict, default = None\n",
      "        Dictionary of tag_name: String -> value: (String, but will be string-ified\n",
      "        if not) passed to the mlflow.set_tags to add new custom tags for the experiment.\n",
      "    \n",
      "    \n",
      "    probability_threshold: float, default = None\n",
      "        Threshold for converting predicted probability to class label.\n",
      "        It defaults to 0.5 for all classifiers unless explicitly defined\n",
      "        in this parameter. Only applicable for binary classification.\n",
      "    \n",
      "    \n",
      "    engine: Optional[Dict[str, str]] = None\n",
      "        The execution engines to use for the models in the form of a dict\n",
      "        of `model_id: engine` - e.g. for Logistic Regression (\"lr\", users can\n",
      "        switch between \"sklearn\" and \"sklearnex\" by specifying\n",
      "        `engine={\"lr\": \"sklearnex\"}`\n",
      "    \n",
      "    \n",
      "    verbose: bool, default = True\n",
      "        Score grid is not printed when verbose is set to False.\n",
      "    \n",
      "    \n",
      "    parallel: pycaret.internal.parallel.parallel_backend.ParallelBackend, default = None\n",
      "        A ParallelBackend instance. For example if you have a SparkSession ``session``,\n",
      "        you can use ``FugueBackend(session)`` to make this function running using\n",
      "        Spark. For more details, see\n",
      "        :class:`~pycaret.parallel.fugue_backend.FugueBackend`\n",
      "    \n",
      "    Returns:\n",
      "        Trained model or list of trained models, depending on the ``n_select`` param.\n",
      "    \n",
      "    \n",
      "    Warnings\n",
      "    --------\n",
      "    - Changing turbo parameter to False may result in very high training times with\n",
      "      datasets exceeding 10,000 rows.\n",
      "    \n",
      "    - AUC for estimators that does not support 'predict_proba' is shown as 0.0000.\n",
      "    \n",
      "    - No models are logged in ``MLFlow`` when ``cross_validation`` parameter is False.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(compare_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43cea6ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function setup in module pycaret.classification.functional:\n",
      "\n",
      "setup(data: Union[dict, list, tuple, numpy.ndarray, scipy.sparse._matrix.spmatrix, pandas.core.frame.DataFrame, NoneType] = None, data_func: Optional[Callable[[], Union[dict, list, tuple, numpy.ndarray, scipy.sparse._matrix.spmatrix, pandas.core.frame.DataFrame]]] = None, target: Union[int, str, list, tuple, numpy.ndarray, pandas.core.series.Series] = -1, index: Union[bool, int, str, list, tuple, numpy.ndarray, pandas.core.series.Series] = True, train_size: float = 0.7, test_data: Union[dict, list, tuple, numpy.ndarray, scipy.sparse._matrix.spmatrix, pandas.core.frame.DataFrame, NoneType] = None, ordinal_features: Optional[Dict[str, list]] = None, numeric_features: Optional[List[str]] = None, categorical_features: Optional[List[str]] = None, date_features: Optional[List[str]] = None, text_features: Optional[List[str]] = None, ignore_features: Optional[List[str]] = None, keep_features: Optional[List[str]] = None, preprocess: bool = True, create_date_columns: List[str] = ['day', 'month', 'year'], imputation_type: Optional[str] = 'simple', numeric_imputation: Union[int, float, str] = 'mean', categorical_imputation: str = 'mode', iterative_imputation_iters: int = 5, numeric_iterative_imputer: Union[str, Any] = 'lightgbm', categorical_iterative_imputer: Union[str, Any] = 'lightgbm', text_features_method: str = 'tf-idf', max_encoding_ohe: int = 25, encoding_method: Optional[Any] = None, rare_to_value: Optional[float] = None, rare_value: str = 'rare', polynomial_features: bool = False, polynomial_degree: int = 2, low_variance_threshold: Optional[float] = None, group_features: Optional[list] = None, group_names: Union[str, list, NoneType] = None, drop_groups: bool = False, remove_multicollinearity: bool = False, multicollinearity_threshold: float = 0.9, bin_numeric_features: Optional[List[str]] = None, remove_outliers: bool = False, outliers_method: str = 'iforest', outliers_threshold: float = 0.05, fix_imbalance: bool = False, fix_imbalance_method: Union[str, Any] = 'SMOTE', transformation: bool = False, transformation_method: str = 'yeo-johnson', normalize: bool = False, normalize_method: str = 'zscore', pca: bool = False, pca_method: str = 'linear', pca_components: Union[int, float, str, NoneType] = None, feature_selection: bool = False, feature_selection_method: str = 'classic', feature_selection_estimator: Union[str, Any] = 'lightgbm', n_features_to_select: Union[int, float] = 0.2, custom_pipeline: Optional[Any] = None, custom_pipeline_position: int = -1, data_split_shuffle: bool = True, data_split_stratify: Union[bool, List[str]] = True, fold_strategy: Union[str, Any] = 'stratifiedkfold', fold: int = 10, fold_shuffle: bool = False, fold_groups: Union[str, pandas.core.frame.DataFrame, NoneType] = None, n_jobs: Optional[int] = -1, use_gpu: bool = False, html: bool = True, session_id: Optional[int] = None, system_log: Union[bool, str, logging.Logger] = True, log_experiment: Union[bool, str, pycaret.loggers.base_logger.BaseLogger, List[Union[str, pycaret.loggers.base_logger.BaseLogger]]] = False, experiment_name: Optional[str] = None, experiment_custom_tags: Optional[Dict[str, Any]] = None, log_plots: Union[bool, list] = False, log_profile: bool = False, log_data: bool = False, verbose: bool = True, memory: Union[bool, str, joblib.memory.Memory] = True, profile: bool = False, profile_kwargs: Optional[Dict[str, Any]] = None)\n",
      "    This function initializes the training environment and creates the transformation\n",
      "    pipeline. Setup function must be called before executing any other function. It takes\n",
      "    two mandatory parameters: ``data`` and ``target``. All the other parameters are\n",
      "    optional.\n",
      "    \n",
      "    Example\n",
      "    -------\n",
      "    >>> from pycaret.datasets import get_data\n",
      "    >>> juice = get_data('juice')\n",
      "    >>> from pycaret.classification import *\n",
      "    >>> exp_name = setup(data = juice,  target = 'Purchase')\n",
      "    \n",
      "    \n",
      "    data: dataframe-like = None\n",
      "        Data set with shape (n_samples, n_features), where n_samples is the\n",
      "        number of samples and n_features is the number of features. If data\n",
      "        is not a pandas dataframe, it's converted to one using default column\n",
      "        names.\n",
      "    \n",
      "    \n",
      "    data_func: Callable[[], DATAFRAME_LIKE] = None\n",
      "        The function that generate ``data`` (the dataframe-like input). This\n",
      "        is useful when the dataset is large, and you need parallel operations\n",
      "        such as ``compare_models``. It can avoid broadcasting large dataset\n",
      "        from driver to workers. Notice one and only one of ``data`` and\n",
      "        ``data_func`` must be set.\n",
      "    \n",
      "    \n",
      "    target: int, str or sequence, default = -1\n",
      "        If int or str, respectivcely index or name of the target column in data.\n",
      "        The default value selects the last column in the dataset. If sequence,\n",
      "        it should have shape (n_samples,). The target can be either binary or\n",
      "        multiclass.\n",
      "    \n",
      "    \n",
      "    index: bool, int, str or sequence, default = True\n",
      "        Handle indices in the `data` dataframe.\n",
      "            - If False: Reset to RangeIndex.\n",
      "            - If True: Keep the provided index.\n",
      "            - If int: Position of the column to use as index.\n",
      "            - If str: Name of the column to use as index.\n",
      "            - If sequence: Array with shape=(n_samples,) to use as index.\n",
      "    \n",
      "    \n",
      "    train_size: float, default = 0.7\n",
      "        Proportion of the dataset to be used for training and validation. Should be\n",
      "        between 0.0 and 1.0.\n",
      "    \n",
      "    \n",
      "    test_data: dataframe-like or None, default = None\n",
      "        If not None, test_data is used as a hold-out set and `train_size` parameter\n",
      "        is ignored. The columns of data and test_data must match.\n",
      "    \n",
      "    \n",
      "    ordinal_features: dict, default = None\n",
      "        Categorical features to be encoded ordinally. For example, a categorical\n",
      "        feature with 'low', 'medium', 'high' values where low < medium < high can\n",
      "        be passed as ordinal_features = {'column_name' : ['low', 'medium', 'high']}.\n",
      "    \n",
      "    \n",
      "    numeric_features: list of str, default = None\n",
      "        If the inferred data types are not correct, the numeric_features param can\n",
      "        be used to define the data types. It takes a list of strings with column\n",
      "        names that are numeric.\n",
      "    \n",
      "    \n",
      "    categorical_features: list of str, default = None\n",
      "        If the inferred data types are not correct, the categorical_features param\n",
      "        can be used to define the data types. It takes a list of strings with column\n",
      "        names that are categorical.\n",
      "    \n",
      "    \n",
      "    date_features: list of str, default = None\n",
      "        If the inferred data types are not correct, the date_features param can be\n",
      "        used to overwrite the data types. It takes a list of strings with column\n",
      "        names that are DateTime.\n",
      "    \n",
      "    \n",
      "    text_features: list of str, default = None\n",
      "        Column names that contain a text corpus. If None, no text features are\n",
      "        selected.\n",
      "    \n",
      "    \n",
      "    ignore_features: list of str, default = None\n",
      "        ignore_features param can be used to ignore features during preprocessing\n",
      "        and model training. It takes a list of strings with column names that are\n",
      "        to be ignored.\n",
      "    \n",
      "    \n",
      "    keep_features: list of str, default = None\n",
      "        keep_features param can be used to always keep specific features during\n",
      "        preprocessing, i.e. these features are never dropped by any kind of\n",
      "        feature selection. It takes a list of strings with column names that are\n",
      "        to be kept.\n",
      "    \n",
      "    \n",
      "    preprocess: bool, default = True\n",
      "        When set to False, no transformations are applied except for train_test_split\n",
      "        and custom transformations passed in ``custom_pipeline`` param. Data must be\n",
      "        ready for modeling (no missing values, no dates, categorical data encoding),\n",
      "        when preprocess is set to False.\n",
      "    \n",
      "    \n",
      "    create_date_columns: list of str, default = [\"day\", \"month\", \"year\"]\n",
      "        Columns to create from the date features. Note that created features\n",
      "        with zero variance (e.g. the feature hour in a column that only contains\n",
      "        dates) are ignored. Allowed values are datetime attributes from\n",
      "        `pandas.Series.dt`. The datetime format of the feature is inferred\n",
      "        automatically from the first non NaN value.\n",
      "    \n",
      "    \n",
      "    imputation_type: str or None, default = 'simple'\n",
      "        The type of imputation to use. Can be either 'simple' or 'iterative'.\n",
      "        If None, no imputation of missing values is performed.\n",
      "    \n",
      "    \n",
      "    numeric_imputation: int, float or str, default = 'mean'\n",
      "        Imputing strategy for numerical columns. Ignored when ``imputation_type=\n",
      "        iterative``. Choose from:\n",
      "            - \"drop\": Drop rows containing missing values.\n",
      "            - \"mean\": Impute with mean of column.\n",
      "            - \"median\": Impute with median of column.\n",
      "            - \"mode\": Impute with most frequent value.\n",
      "            - \"knn\": Impute using a K-Nearest Neighbors approach.\n",
      "            - int or float: Impute with provided numerical value.\n",
      "    \n",
      "    \n",
      "    categorical_imputation: str, default = 'mode'\n",
      "        Imputing strategy for categorical columns. Ignored when ``imputation_type=\n",
      "        iterative``. Choose from:\n",
      "            - \"drop\": Drop rows containing missing values.\n",
      "            - \"mode\": Impute with most frequent value.\n",
      "            - str: Impute with provided string.\n",
      "    \n",
      "    \n",
      "    iterative_imputation_iters: int, default = 5\n",
      "        Number of iterations. Ignored when ``imputation_type=simple``.\n",
      "    \n",
      "    \n",
      "    numeric_iterative_imputer: str or sklearn estimator, default = 'lightgbm'\n",
      "        Regressor for iterative imputation of missing values in numeric features.\n",
      "        If None, it uses LGBClassifier. Ignored when ``imputation_type=simple``.\n",
      "    \n",
      "    \n",
      "    categorical_iterative_imputer: str or sklearn estimator, default = 'lightgbm'\n",
      "        Regressor for iterative imputation of missing values in categorical features.\n",
      "        If None, it uses LGBClassifier. Ignored when ``imputation_type=simple``.\n",
      "    \n",
      "    \n",
      "    text_features_method: str, default = \"tf-idf\"\n",
      "        Method with which to embed the text features in the dataset. Choose\n",
      "        between \"bow\" (Bag of Words - CountVectorizer) or \"tf-idf\" (TfidfVectorizer).\n",
      "        Be aware that the sparse matrix output of the transformer is converted\n",
      "        internally to its full array. This can cause memory issues for large\n",
      "        text embeddings.\n",
      "    \n",
      "    \n",
      "    max_encoding_ohe: int, default = 25\n",
      "        Categorical columns with `max_encoding_ohe` or less unique values are\n",
      "        encoded using OneHotEncoding. If more, the `encoding_method` estimator\n",
      "        is used. Note that columns with exactly two classes are always encoded\n",
      "        ordinally. Set to below 0 to always use OneHotEncoding.\n",
      "    \n",
      "    \n",
      "    encoding_method: category-encoders estimator, default = None\n",
      "        A `category-encoders` estimator to encode the categorical columns\n",
      "        with more than `max_encoding_ohe` unique values. If None,\n",
      "        `category_encoders.target_encoder.TargetEncoder` is used.\n",
      "    \n",
      "    \n",
      "    rare_to_value: float or None, default=None\n",
      "        Minimum fraction of category occurrences in a categorical column.\n",
      "        If a category is less frequent than `rare_to_value * len(X)`, it is\n",
      "        replaced with the string in `rare_value`. Use this parameter to group\n",
      "        rare categories before encoding the column. If None, ignores this step.\n",
      "    \n",
      "    \n",
      "    rare_value: str, default=\"rare\"\n",
      "        Value with which to replace rare categories. Ignored when\n",
      "        ``rare_to_value`` is None.\n",
      "    \n",
      "    \n",
      "    polynomial_features: bool, default = False\n",
      "        When set to True, new features are derived using existing numeric features.\n",
      "    \n",
      "    \n",
      "    polynomial_degree: int, default = 2\n",
      "        Degree of polynomial features. For example, if an input sample is two dimensional\n",
      "        and of the form [a, b], the polynomial features with degree = 2 are:\n",
      "        [1, a, b, a^2, ab, b^2]. Ignored when ``polynomial_features`` is not True.\n",
      "    \n",
      "    \n",
      "    low_variance_threshold: float or None, default = None\n",
      "        Remove features with a training-set variance lower than the provided\n",
      "        threshold. If 0, keep all features with non-zero variance, i.e. remove\n",
      "        the features that have the same value in all samples. If None, skip\n",
      "        this transformation step.\n",
      "    \n",
      "    \n",
      "    group_features: list, list of lists or None, default = None\n",
      "        When the dataset contains features with related characteristics,\n",
      "        add new fetaures with the following statistical properties of that\n",
      "        group: min, max, mean, std, median and mode. The parameter takes a\n",
      "        list of feature names or a list of lists of feature names to specify\n",
      "        multiple groups.\n",
      "    \n",
      "    group_names: str, list, or None, default = None\n",
      "        Group names to be used when naming the new features. The length\n",
      "        should match with the number of groups specified in ``group_features``.\n",
      "        If None, new features are named using the default form, e.g. group_1,\n",
      "        group_2, etc... Ignored when ``group_features`` is None.\n",
      "    \n",
      "    \n",
      "    drop_groups: bool, default=False\n",
      "        Whether to drop the original features in the group. Ignored when\n",
      "        ``group_features`` is None.\n",
      "    \n",
      "    \n",
      "    remove_multicollinearity: bool, default = False\n",
      "        When set to True, features with the inter-correlations higher than\n",
      "        the defined threshold are removed. For each group, it removes all\n",
      "        except the feature with the highest correlation to `y`.\n",
      "    \n",
      "    \n",
      "    multicollinearity_threshold: float, default = 0.9\n",
      "        Minimum absolute Pearson correlation to identify correlated\n",
      "        features. The default value removes equal columns. Ignored when\n",
      "        ``remove_multicollinearity`` is not True.\n",
      "    \n",
      "    \n",
      "    bin_numeric_features: list of str, default = None\n",
      "        To convert numeric features into categorical, bin_numeric_features parameter can\n",
      "        be used. It takes a list of strings with column names to be discretized. It does\n",
      "        so by using 'sturges' rule to determine the number of clusters and then apply\n",
      "        KMeans algorithm. Original values of the feature are then replaced by the\n",
      "        cluster label.\n",
      "    \n",
      "    \n",
      "    remove_outliers: bool, default = False\n",
      "        When set to True, outliers from the training data are removed using an\n",
      "        Isolation Forest.\n",
      "    \n",
      "    \n",
      "    outliers_method: str, default = \"iforest\"\n",
      "        Method with which to remove outliers. Ignored when `remove_outliers=False`.\n",
      "        Possible values are:\n",
      "            - 'iforest': Uses sklearn's IsolationForest.\n",
      "            - 'ee': Uses sklearn's EllipticEnvelope.\n",
      "            - 'lof': Uses sklearn's LocalOutlierFactor.\n",
      "    \n",
      "    \n",
      "    outliers_threshold: float, default = 0.05\n",
      "        The percentage of outliers to be removed from the dataset. Ignored\n",
      "        when ``remove_outliers=False``.\n",
      "    \n",
      "    \n",
      "    fix_imbalance: bool, default = False\n",
      "        When training dataset has unequal distribution of target class it can be balanced\n",
      "        using this parameter. When set to True, SMOTE (Synthetic Minority Over-sampling\n",
      "        Technique) is applied by default to create synthetic datapoints for minority class.\n",
      "    \n",
      "    \n",
      "    fix_imbalance_method: str or imblearn estimator, default = \"SMOTE\"\n",
      "        Estimator with which to perform class balancing. Choose from the name\n",
      "        of an `imblearn` estimator, or a custom instance of such. Ignored when\n",
      "        `fix_imbalance=False`.\n",
      "    \n",
      "    \n",
      "    transformation: bool, default = False\n",
      "        When set to True, it applies the power transform to make data more Gaussian-like.\n",
      "        Type of transformation is defined by the ``transformation_method`` parameter.\n",
      "    \n",
      "    \n",
      "    transformation_method: str, default = 'yeo-johnson'\n",
      "        Defines the method for transformation. By default, the transformation method is\n",
      "        set to 'yeo-johnson'. The other available option for transformation is 'quantile'.\n",
      "        Ignored when ``transformation`` is not True.\n",
      "    \n",
      "    \n",
      "    normalize: bool, default = False\n",
      "        When set to True, it transforms the features by scaling them to a given\n",
      "        range. Type of scaling is defined by the ``normalize_method`` parameter.\n",
      "    \n",
      "    \n",
      "    normalize_method: str, default = 'zscore'\n",
      "        Defines the method for scaling. By default, normalize method is set to 'zscore'\n",
      "        The standard zscore is calculated as z = (x - u) / s. Ignored when ``normalize``\n",
      "        is not True. The other options are:\n",
      "    \n",
      "        - minmax: scales and translates each feature individually such that it is in\n",
      "          the range of 0 - 1.\n",
      "        - maxabs: scales and translates each feature individually such that the\n",
      "          maximal absolute value of each feature will be 1.0. It does not\n",
      "          shift/center the data, and thus does not destroy any sparsity.\n",
      "        - robust: scales and translates each feature according to the Interquartile\n",
      "          range. When the dataset contains outliers, robust scaler often gives\n",
      "          better results.\n",
      "    \n",
      "    \n",
      "    pca: bool, default = False\n",
      "        When set to True, dimensionality reduction is applied to project the data into\n",
      "        a lower dimensional space using the method defined in ``pca_method`` parameter.\n",
      "    \n",
      "    \n",
      "    pca_method: str, default = 'linear'\n",
      "        Method with which to apply PCA. Possible values are:\n",
      "            - 'linear': Uses Singular Value  Decomposition.\n",
      "            - 'kernel': Dimensionality reduction through the use of RBF kernel.\n",
      "            - 'incremental': Similar to 'linear', but more efficient for large datasets.\n",
      "    \n",
      "    \n",
      "    pca_components: int, float, str or None, default = None\n",
      "        Number of components to keep. This parameter is ignored when `pca=False`.\n",
      "            - If None: All components are kept.\n",
      "            - If int: Absolute number of components.\n",
      "            - If float: Such an amount that the variance that needs to be explained\n",
      "                        is greater than the percentage specified by `n_components`.\n",
      "                        Value should lie between 0 and 1 (ony for pca_method='linear').\n",
      "            - If \"mle\": Minkaâ€™s MLE is used to guess the dimension (ony for pca_method='linear').\n",
      "    \n",
      "    \n",
      "    feature_selection: bool, default = False\n",
      "        When set to True, a subset of features is selected based on a feature\n",
      "        importance score determined by ``feature_selection_estimator``.\n",
      "    \n",
      "    \n",
      "    feature_selection_method: str, default = 'classic'\n",
      "        Algorithm for feature selection. Choose from:\n",
      "            - 'univariate': Uses sklearn's SelectKBest.\n",
      "            - 'classic': Uses sklearn's SelectFromModel.\n",
      "            - 'sequential': Uses sklearn's SequentialFeatureSelector.\n",
      "    \n",
      "    \n",
      "    feature_selection_estimator: str or sklearn estimator, default = 'lightgbm'\n",
      "        Classifier used to determine the feature importances. The\n",
      "        estimator should have a `feature_importances_` or `coef_`\n",
      "        attribute after fitting. If None, it uses LGBClassifier. This\n",
      "        parameter is ignored when `feature_selection_method=univariate`.\n",
      "    \n",
      "    \n",
      "    n_features_to_select: int or float, default = 0.2\n",
      "        The maximum number of features to select with feature_selection. If <1,\n",
      "        it's the fraction of starting features. Note that this parameter doesn't\n",
      "        take features in ``ignore_features`` or ``keep_features`` into account\n",
      "        when counting.\n",
      "    \n",
      "    \n",
      "    custom_pipeline: list of (str, transformer), dict or Pipeline, default = None\n",
      "        Addidiotnal custom transformers. If passed, they are applied to the\n",
      "        pipeline last, after all the build-in transformers.\n",
      "    \n",
      "    \n",
      "    custom_pipeline_position: int, default = -1\n",
      "        Position of the custom pipeline in the overal preprocessing pipeline.\n",
      "        The default value adds the custom pipeline last.\n",
      "    \n",
      "    \n",
      "    data_split_shuffle: bool, default = True\n",
      "        When set to False, prevents shuffling of rows during 'train_test_split'.\n",
      "    \n",
      "    \n",
      "    data_split_stratify: bool or list, default = True\n",
      "        Controls stratification during 'train_test_split'. When set to True, will\n",
      "        stratify by target column. To stratify on any other columns, pass a list of\n",
      "        column names. Ignored when ``data_split_shuffle`` is False.\n",
      "    \n",
      "    \n",
      "    fold_strategy: str or sklearn CV generator object, default = 'stratifiedkfold'\n",
      "        Choice of cross validation strategy. Possible values are:\n",
      "    \n",
      "        * 'kfold'\n",
      "        * 'stratifiedkfold'\n",
      "        * 'groupkfold'\n",
      "        * 'timeseries'\n",
      "        * a custom CV generator object compatible with scikit-learn.\n",
      "    \n",
      "        For ``groupkfold``, column name must be passed in ``fold_groups`` parameter.\n",
      "        Example: ``setup(fold_strategy=\"groupkfold\", fold_groups=\"COLUMN_NAME\")``\n",
      "    \n",
      "    fold: int, default = 10\n",
      "        Number of folds to be used in cross validation. Must be at least 2. This is\n",
      "        a global setting that can be over-written at function level by using ``fold``\n",
      "        parameter. Ignored when ``fold_strategy`` is a custom object.\n",
      "    \n",
      "    \n",
      "    fold_shuffle: bool, default = False\n",
      "        Controls the shuffle parameter of CV. Only applicable when ``fold_strategy``\n",
      "        is 'kfold' or 'stratifiedkfold'. Ignored when ``fold_strategy`` is a custom\n",
      "        object.\n",
      "    \n",
      "    \n",
      "    fold_groups: str or array-like, with shape (n_samples,), default = None\n",
      "        Optional group labels when 'GroupKFold' is used for the cross validation.\n",
      "        It takes an array with shape (n_samples, ) where n_samples is the number\n",
      "        of rows in the training dataset. When string is passed, it is interpreted\n",
      "        as the column name in the dataset containing group labels.\n",
      "    \n",
      "    \n",
      "    n_jobs: int, default = -1\n",
      "        The number of jobs to run in parallel (for functions that supports parallel\n",
      "        processing) -1 means using all processors. To run all functions on single\n",
      "        processor set n_jobs to None.\n",
      "    \n",
      "    \n",
      "    use_gpu: bool or str, default = False\n",
      "        When set to True, it will use GPU for training with algorithms that support it,\n",
      "        and fall back to CPU if they are unavailable. When set to 'force', it will only\n",
      "        use GPU-enabled algorithms and raise exceptions when they are unavailable. When\n",
      "        False, all algorithms are trained using CPU only.\n",
      "    \n",
      "        GPU enabled algorithms:\n",
      "    \n",
      "        - Extreme Gradient Boosting, requires no further installation\n",
      "    \n",
      "        - CatBoost Classifier, requires no further installation\n",
      "          (GPU is only enabled when data > 50,000 rows)\n",
      "    \n",
      "        - Light Gradient Boosting Machine, requires GPU installation\n",
      "          https://lightgbm.readthedocs.io/en/latest/GPU-Tutorial.html\n",
      "    \n",
      "        - Logistic Regression, Ridge Classifier, Random Forest, K Neighbors Classifier,\n",
      "          Support Vector Machine, requires cuML >= 0.15\n",
      "          https://github.com/rapidsai/cuml\n",
      "    \n",
      "    \n",
      "    html: bool, default = True\n",
      "        When set to False, prevents runtime display of monitor. This must be set to False\n",
      "        when the environment does not support IPython. For example, command line terminal,\n",
      "        Databricks Notebook, Spyder and other similar IDEs.\n",
      "    \n",
      "    \n",
      "    session_id: int, default = None\n",
      "        Controls the randomness of experiment. It is equivalent to 'random_state' in\n",
      "        scikit-learn. When None, a pseudo random number is generated. This can be used\n",
      "        for later reproducibility of the entire experiment.\n",
      "    \n",
      "    \n",
      "    log_experiment: bool or str or BaseLogger or list of str or BaseLogger, default = False\n",
      "        A (list of) PyCaret ``BaseLogger`` or str (one of 'mlflow', 'wandb', 'comet_ml')\n",
      "        corresponding to a logger to determine which experiment loggers to use.\n",
      "        Setting to True will use just MLFlow.\n",
      "    \n",
      "    \n",
      "    system_log: bool or str or logging.Logger, default = True\n",
      "        Whether to save the system logging file (as logs.log). If the input\n",
      "        is a string, use that as the path to the logging file. If the input\n",
      "        already is a logger object, use that one instead.\n",
      "    \n",
      "    \n",
      "    experiment_name: str, default = None\n",
      "        Name of the experiment for logging. Ignored when ``log_experiment`` is False.\n",
      "    \n",
      "    \n",
      "    experiment_custom_tags: dict, default = None\n",
      "        Dictionary of tag_name: String -> value: (String, but will be string-ified\n",
      "        if not) passed to the mlflow.set_tags to add new custom tags for the experiment.\n",
      "    \n",
      "    \n",
      "    log_plots: bool or list, default = False\n",
      "        When set to True, certain plots are logged automatically in the ``MLFlow`` server.\n",
      "        To change the type of plots to be logged, pass a list containing plot IDs. Refer\n",
      "        to documentation of ``plot_model``. Ignored when ``log_experiment`` is False.\n",
      "    \n",
      "    \n",
      "    log_profile: bool, default = False\n",
      "        When set to True, data profile is logged on the ``MLflow`` server as a html file.\n",
      "        Ignored when ``log_experiment`` is False.\n",
      "    \n",
      "    \n",
      "    log_data: bool, default = False\n",
      "        When set to True, dataset is logged on the ``MLflow`` server as a csv file.\n",
      "        Ignored when ``log_experiment`` is False.\n",
      "    \n",
      "    \n",
      "    verbose: bool, default = True\n",
      "        When set to False, Information grid is not printed.\n",
      "    \n",
      "    \n",
      "    memory: str, bool or Memory, default=True\n",
      "        Used to cache the fitted transformers of the pipeline.\n",
      "            If False: No caching is performed.\n",
      "            If True: A default temp directory is used.\n",
      "            If str: Path to the caching directory.\n",
      "    \n",
      "    \n",
      "    profile: bool, default = False\n",
      "        When set to True, an interactive EDA report is displayed.\n",
      "    \n",
      "    \n",
      "    profile_kwargs: dict, default = {} (empty dict)\n",
      "        Dictionary of arguments passed to the ProfileReport method used\n",
      "        to create the EDA report. Ignored if ``profile`` is False.\n",
      "    \n",
      "    \n",
      "    Returns:\n",
      "        ClassificationExperiment object.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(setup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ccae59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
